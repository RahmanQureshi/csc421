{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "outputId": "58d6bece-c9c8-4072-d3dd-afd9140ad3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python2.7/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torch) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python2.7/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.14.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.3.0 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 4.3.0\n",
            "    Uninstalling Pillow-4.3.0:\n",
            "      Successfully uninstalled Pillow-4.3.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXNsLNkOn38w",
        "colab_type": "text"
      },
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BAfi_8yWB3y",
        "colab_type": "text"
      },
      "source": [
        "## GRU cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ztmyA5Ro67o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.parameter.Parameter(torch.rand(hidden_size, input_size))\n",
        "        self.Wir = nn.parameter.Parameter(torch.rand(hidden_size, input_size))\n",
        "        self.Wih = nn.parameter.Parameter(torch.rand(hidden_size, input_size))\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.parameter.Parameter(torch.rand(hidden_size, hidden_size))\n",
        "        self.Whr = nn.parameter.Parameter(torch.rand(hidden_size, hidden_size))\n",
        "        self.Whh = nn.parameter.Parameter(torch.rand(hidden_size, hidden_size))\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # h is stored as a row vector\n",
        "        z = nn.functional.sigmoid(torch.mm(x, self.Wiz.t()) + torch.mm(self.Whz, h_prev.t()).t())\n",
        "        r = nn.functional.sigmoid(torch.mm(x, self.Wir.t()) + torch.mm(self.Whr, h_prev.t()).t())\n",
        "        g = nn.functional.tanh(torch.mm(x, self.Wih.t()) + r * torch.mm(self.Whh, h_prev.t()).t())\n",
        "        h_new = (1-z) * g + z * h_prev;\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JBVFLEZWNC1",
        "colab_type": "text"
      },
      "source": [
        "### GRU encoder / decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaDt7XDmWRzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWe0RO5FWajD",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GUK5A7CWhV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = queries.size()[0]\n",
        "        seq_len = keys.size()[1]\n",
        "        expanded_queries = queries.unsqueeze(1).expand(batch_size, seq_len, self.hidden_size) # batch_size x seq_len x hidden_size\n",
        "        concat_inputs = torch.cat((expanded_queries, keys), 2) # batch_size x seq_len x 2*hidden_size\n",
        "        unnormalized_attention = self.attention_network(concat_inputs).view(batch_size, 1, seq_len) # batch_size x seq_len x 1 -> batch_size x 1 x seq_len.\n",
        "        attention_weights = self.softmax(unnormalized_attention) # batch_size x seq_len x 1\n",
        "        context = torch.bmm(attention_weights, values)\n",
        "        return context, attention_weights\n",
        "        \n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights\n",
        "        \n",
        "\n",
        "      \n",
        "      \n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # mask = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pemjZo2XWtRt",
        "colab_type": "text"
      },
      "source": [
        "### Attention decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfjF0Z-PWwPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN\n",
        "            # ------------\n",
        "            embed_current = embed[:, i, :] # batch_size x hidden_size\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations) # context = batch_size x 1 x hidden_size\n",
        "            context = context.squeeze(1) # batch_size x hidden_size\n",
        "            embed_and_context = torch.cat((embed_current, context), 1)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)\n",
        "\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8JpcwTRW5cw",
        "colab_type": "text"
      },
      "source": [
        "### Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5vJPku1W7sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN\n",
        "          # ------------\n",
        "          # new_contexts, self_attention_weights = ...\n",
        "          # residual_contexts = ...\n",
        "          # new_contexts, encoder_attention_weights = ...\n",
        "          # residual_contexts = ...\n",
        "          # new_contexts = ...\n",
        "          # contexts = ...\n",
        "\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNFd6LNo0-o",
        "colab_type": "text"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiUwiOITHTW4",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwcFjsEpHRbI",
        "colab_type": "code",
        "outputId": "de8f75fe-21bd-49ff-dd00-356d7254dc93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n",
            "('Downloading data from', 'http://www.cs.toronto.edu/~jba/pig_latin_data.txt')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQmyJDSRFKR",
        "colab_type": "text"
      },
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "colab_type": "code",
        "outputId": "eda020ce-6c14-4d63-8b90-80ea3698c7c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.440 | Val loss: 2.045 | Gen: eray eray engay engay engay\n",
            "Epoch:   1 | Train loss: 1.970 | Val loss: 1.893 | Gen: eray areray ongeray estay ongeray\n",
            "Epoch:   2 | Train loss: 1.817 | Val loss: 1.804 | Gen: eray arertay ongersay angedway ongestay\n",
            "Epoch:   3 | Train loss: 1.716 | Val loss: 1.728 | Gen: erthay aredway ontersay ingsay ontersay\n",
            "Epoch:   4 | Train loss: 1.640 | Val loss: 1.673 | Gen: erthay aredway outingstay istay onterstay\n",
            "Epoch:   5 | Train loss: 1.580 | Val loss: 1.641 | Gen: erthay aredway ontingstay ingsay onterstay\n",
            "Epoch:   6 | Train loss: 1.526 | Val loss: 1.603 | Gen: erthay aredway outiongsay inglay outingstay\n",
            "Epoch:   7 | Train loss: 1.474 | Val loss: 1.543 | Gen: estay away ontingstay iseday outiongway\n",
            "Epoch:   8 | Train loss: 1.432 | Val loss: 1.506 | Gen: ertay ayeay ontingsay iseday outiongway\n",
            "Epoch:   9 | Train loss: 1.380 | Val loss: 1.474 | Gen: erthay aingway ontingsay iseday onencentsay\n",
            "Epoch:  10 | Train loss: 1.343 | Val loss: 1.468 | Gen: eray aiesway ontingsay iseday ortingsay\n",
            "Epoch:  11 | Train loss: 1.312 | Val loss: 1.427 | Gen: estay aingway ontingstay iseday ortingsay\n",
            "Epoch:  12 | Train loss: 1.295 | Val loss: 1.413 | Gen: etay aiesway ontingsay iseday orterredway\n",
            "Epoch:  13 | Train loss: 1.262 | Val loss: 1.392 | Gen: estay aiessay ontingstay iseday ornenceway\n",
            "Epoch:  14 | Train loss: 1.228 | Val loss: 1.376 | Gen: estay aiessay ontingsay iseday oncenceway\n",
            "Epoch:  15 | Train loss: 1.210 | Val loss: 1.364 | Gen: estay aingway oningntedsay iseday ortingsay\n",
            "Epoch:  16 | Train loss: 1.186 | Val loss: 1.350 | Gen: estay aiedway onintingsay isay oncenceway\n",
            "Epoch:  17 | Train loss: 1.154 | Val loss: 1.324 | Gen: estay aingway onintingsssay istay oncencestay\n",
            "Epoch:  18 | Train loss: 1.139 | Val loss: 1.317 | Gen: estay aiedway onintingsay isay oncencestay\n",
            "Epoch:  19 | Train loss: 1.136 | Val loss: 1.323 | Gen: ethay aingway onintingsay isay oncenceway\n",
            "Epoch:  20 | Train loss: 1.134 | Val loss: 1.295 | Gen: ethay aingway onintingday isay oncingray\n",
            "Epoch:  21 | Train loss: 1.111 | Val loss: 1.313 | Gen: ethay aingway onintingday isay oncenceway\n",
            "Epoch:  22 | Train loss: 1.093 | Val loss: 1.276 | Gen: ethay aingway oninsingsay isay onsingray\n",
            "Epoch:  23 | Train loss: 1.068 | Val loss: 1.287 | Gen: ethay aingway oninsingstay istay onsingredway\n",
            "Epoch:  24 | Train loss: 1.053 | Val loss: 1.249 | Gen: ethay aiedday oninsingstay istay onsincerpay\n",
            "Epoch:  25 | Train loss: 1.041 | Val loss: 1.238 | Gen: ethay aiedday oninsingstay isay onsingray\n",
            "Epoch:  26 | Train loss: 1.023 | Val loss: 1.239 | Gen: ethay aingway oninsingstay iseday onsingray\n",
            "Epoch:  27 | Train loss: 1.013 | Val loss: 1.223 | Gen: ethay aingway oninsingstay iseday onsinsingsay\n",
            "Epoch:  28 | Train loss: 1.000 | Val loss: 1.221 | Gen: ethay aingway oninsinghay-ingshay isay orsingray\n",
            "Epoch:  29 | Train loss: 1.001 | Val loss: 1.217 | Gen: ethay aingway onintingfay isay onsingray\n",
            "Epoch:  30 | Train loss: 0.985 | Val loss: 1.213 | Gen: ethay aingway oninstingday iseday onsingray-inway-ayda\n",
            "Epoch:  31 | Train loss: 0.978 | Val loss: 1.187 | Gen: ethay aingway oninsinghtay isedway onsingray\n",
            "Epoch:  32 | Train loss: 0.959 | Val loss: 1.209 | Gen: ethay aingway onintingray-inway-aw isay onsingray\n",
            "Epoch:  33 | Train loss: 0.992 | Val loss: 1.192 | Gen: ethay aingway onindingstay-inway-a isedway onsingray\n",
            "Epoch:  34 | Train loss: 0.955 | Val loss: 1.175 | Gen: ethay aingway onintingstay-inway-a issay onsisingray\n",
            "Epoch:  35 | Train loss: 0.935 | Val loss: 1.156 | Gen: ethay aingway oninsinghay-inway-aw isay onsingray\n",
            "Epoch:  36 | Train loss: 0.934 | Val loss: 1.157 | Gen: ethay aingway onintingray-inway-aw isedway onsisingray\n",
            "Epoch:  37 | Train loss: 0.942 | Val loss: 1.164 | Gen: ethay aiedday oninsingstay isay onsiserway\n",
            "Epoch:  38 | Train loss: 0.919 | Val loss: 1.160 | Gen: ethay aiedday onintingstay isday onsingray\n",
            "Epoch:  39 | Train loss: 0.895 | Val loss: 1.159 | Gen: ethay aingway onintishingsay isay onsisedlay\n",
            "Epoch:  40 | Train loss: 0.894 | Val loss: 1.141 | Gen: ethay aingway onintingstay-inway-a issay orkingsay\n",
            "Epoch:  41 | Train loss: 0.915 | Val loss: 1.140 | Gen: ethay aingway onintingtay-inway-aw isday onsisedlay\n",
            "Epoch:  42 | Train loss: 0.892 | Val loss: 1.130 | Gen: ethay aiedday onintingstay-inway-a issay onsisedlay\n",
            "Epoch:  43 | Train loss: 0.870 | Val loss: 1.117 | Gen: ethay aiedday onintingstay-inway-a isedway onsisedlay\n",
            "Epoch:  44 | Train loss: 0.871 | Val loss: 1.119 | Gen: ethay aiedday onintingray-estay-in isday onsisedlay\n",
            "Epoch:  45 | Train loss: 0.882 | Val loss: 1.139 | Gen: ethay aiedday onintingstay isday onsisedlay\n",
            "Epoch:  46 | Train loss: 0.861 | Val loss: 1.121 | Gen: ethay aiedday onintingstay-inway-a isday orkingday\n",
            "Epoch:  47 | Train loss: 0.877 | Val loss: 1.127 | Gen: ethay aiedcay onintingstay issay orkingday\n",
            "Epoch:  48 | Train loss: 0.862 | Val loss: 1.114 | Gen: ethay aiedday onintingstay issway onsiserway\n",
            "Epoch:  49 | Train loss: 0.842 | Val loss: 1.110 | Gen: ethay aiedcay onintingstay issway onsisedlay\n",
            "Epoch:  50 | Train loss: 0.867 | Val loss: 1.095 | Gen: ethay aiedcay onintingstay-inway-a issay onsiserway\n",
            "Epoch:  51 | Train loss: 0.833 | Val loss: 1.100 | Gen: ethay aiedcay onintingstay-inway-a issway onsisedlay\n",
            "Epoch:  52 | Train loss: 0.828 | Val loss: 1.108 | Gen: ethay aiedcay onintingstay-inway-a isedway onsiserway\n",
            "Epoch:  53 | Train loss: 0.826 | Val loss: 1.093 | Gen: ethay aiedcay onintingstay-inway-a issway onsisedlay\n",
            "Epoch:  54 | Train loss: 0.813 | Val loss: 1.115 | Gen: ethay aiedcay onintingstay isday onsisermay\n",
            "Epoch:  55 | Train loss: 0.827 | Val loss: 1.112 | Gen: ethay aiedcay onintingstay-inway-a issway onsisedlay\n",
            "Epoch:  56 | Train loss: 0.805 | Val loss: 1.097 | Gen: ethay aiedcay onintingstay-inway-a issway onsisermay\n",
            "Epoch:  57 | Train loss: 0.812 | Val loss: 1.092 | Gen: ethay aiedcay onintingstay-inway-a issway onsisedlay\n",
            "Epoch:  58 | Train loss: 0.792 | Val loss: 1.093 | Gen: ethay aiedcay onintingstay-inway-a issway onsisermay\n",
            "Epoch:  59 | Train loss: 0.801 | Val loss: 1.080 | Gen: ethay aiedcay onintingstay-inway-a issway onsiserway\n",
            "Epoch:  60 | Train loss: 0.797 | Val loss: 1.098 | Gen: ethay aiedcay onintingstay-inway-a issway onsiseray\n",
            "Epoch:  61 | Train loss: 0.792 | Val loss: 1.079 | Gen: ethay aiedcay onintingstay-inway-a issway onsiserway\n",
            "Epoch:  62 | Train loss: 0.772 | Val loss: 1.066 | Gen: ethay aiedcay onintingstay-inway-a isedway onsisermay\n",
            "Epoch:  63 | Train loss: 0.790 | Val loss: 1.092 | Gen: ethay aiedcay onintingstay issway onsiseray\n",
            "Epoch:  64 | Train loss: 0.798 | Val loss: 1.080 | Gen: ethay aiedcay onintistanceway isedway orkingsway\n",
            "Epoch:  65 | Train loss: 0.776 | Val loss: 1.058 | Gen: ethay aiedcay onintingstay-inway-a issway onsiserway\n",
            "Epoch:  66 | Train loss: 0.775 | Val loss: 1.065 | Gen: ethay aiedcay onintingstay-inway-a issway onsiseray\n",
            "Epoch:  67 | Train loss: 0.763 | Val loss: 1.052 | Gen: ethay aiedcay onintingstay-inway-a issway onsisermay\n",
            "Epoch:  68 | Train loss: 0.756 | Val loss: 1.043 | Gen: ethay aiedcay onintingstay-inway-a issway onsiseray\n",
            "Epoch:  69 | Train loss: 0.771 | Val loss: 1.077 | Gen: ethay aisedhay incodinteday isedway onsisermay\n",
            "Epoch:  70 | Train loss: 0.757 | Val loss: 1.035 | Gen: ethay aiedcay onintingstay-inway-a isedway onsiseray\n",
            "Epoch:  71 | Train loss: 0.735 | Val loss: 1.047 | Gen: ethay aiescay onintingstay isedway onsiseray\n",
            "Epoch:  72 | Train loss: 0.739 | Val loss: 1.033 | Gen: ethay aiescay onintingstay-inway-a issway onsisermay\n",
            "Epoch:  73 | Train loss: 0.812 | Val loss: 1.083 | Gen: ethay aiedcay incodithanceway isday onsiselay\n",
            "Epoch:  74 | Train loss: 0.779 | Val loss: 1.052 | Gen: ethay aiedcay onintingstay isedway onsiseray\n",
            "Epoch:  75 | Train loss: 0.744 | Val loss: 1.027 | Gen: ethay aiedcay onintingstay-inway-a isedway onsisermay\n",
            "Epoch:  76 | Train loss: 0.727 | Val loss: 1.020 | Gen: ethay aiedcay onintingstay-inway-a issway onsiseray\n",
            "Epoch:  77 | Train loss: 0.718 | Val loss: 1.031 | Gen: ethay aiescay onintingstay issway onsiseray\n",
            "Epoch:  78 | Train loss: 0.711 | Val loss: 1.020 | Gen: ethay aiescay onintingstay-inway-a isedway onsiseray\n",
            "Epoch:  79 | Train loss: 0.729 | Val loss: 1.063 | Gen: ethay airway onintingstay isedway onsiselay\n",
            "Epoch:  80 | Train loss: 0.715 | Val loss: 1.027 | Gen: ethay aiescay onintingstay-inway-a isedway onsisermay\n",
            "Epoch:  81 | Train loss: 0.704 | Val loss: 1.018 | Gen: ethay aiescay onintingstay-inway-a isedway onsiselay\n",
            "Epoch:  82 | Train loss: 0.694 | Val loss: 1.013 | Gen: ethay aiescay onintingstay isedway onsiselay\n",
            "Epoch:  83 | Train loss: 0.694 | Val loss: 1.011 | Gen: ethay aiescay onintingstay isedway onsiselay\n",
            "Epoch:  84 | Train loss: 0.692 | Val loss: 1.009 | Gen: ethay aiescay onintingstay-inway-a isedway onsiselay\n",
            "Epoch:  85 | Train loss: 0.705 | Val loss: 1.035 | Gen: ethay aiescay onintingstay isedway onsiselay\n",
            "Epoch:  86 | Train loss: 0.710 | Val loss: 1.042 | Gen: ethay aiescay onintingstay-inway-a isedway onsiselaray\n",
            "Epoch:  87 | Train loss: 0.710 | Val loss: 1.007 | Gen: ethay aiescay onintingstay-inway-a isedway onseslicepay\n",
            "Epoch:  88 | Train loss: 0.695 | Val loss: 1.008 | Gen: ethay aiescay onintingstay isedway onsisemlay\n",
            "Epoch:  89 | Train loss: 0.681 | Val loss: 1.003 | Gen: ethay aiescay onintingstay isedway onsiseray\n",
            "Epoch:  90 | Train loss: 0.700 | Val loss: 1.028 | Gen: ethay airway onintingstay isedway onsiselay\n",
            "Epoch:  91 | Train loss: 0.712 | Val loss: 1.002 | Gen: ethay aiescay onintintpay-inway-aw isedway onsiselay\n",
            "Epoch:  92 | Train loss: 0.699 | Val loss: 1.008 | Gen: ethay aiescay onintintpationday isedway onsiseray\n",
            "Epoch:  93 | Train loss: 0.691 | Val loss: 1.026 | Gen: ethay aiescay onintingstay-inway-a isedway onseslisgray\n",
            "Epoch:  94 | Train loss: 0.680 | Val loss: 1.017 | Gen: ethay aiescay onintingstay-oday isedway onsiselaray\n",
            "Epoch:  95 | Train loss: 0.682 | Val loss: 0.999 | Gen: ethay aiescay onintingstay isedway onsiseray\n",
            "Epoch:  96 | Train loss: 0.661 | Val loss: 0.997 | Gen: ethay aiescay onintingstay isedway onsiseray\n",
            "Epoch:  97 | Train loss: 0.653 | Val loss: 0.995 | Gen: ethay aiescay onintintpay-astedcay isedway onsiselaray\n",
            "Epoch:  98 | Train loss: 0.660 | Val loss: 0.994 | Gen: ethay aiescay onintingstay isedway onsiselaray\n",
            "Epoch:  99 | Train loss: 0.654 | Val loss: 0.987 | Gen: ethay aiescay onintintpay-enstay isedway onsiselaray\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay aiescay onintintpay-enstay isedway onsiselaray\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2kPGj5DFv7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "\n",
        "TEST_SENTENCE = 'big'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cP7nl5NRJbu",
        "colab_type": "text"
      },
      "source": [
        "## RNN attention decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKlyfbuPDXDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        },
        "outputId": "2bf1c9d1-174f-4d54-d44a-dbce6e020fac"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.849 | Val loss: 2.635 | Gen: eeeeeeeeeeeeayyyyyyy ayyyyyyyyyyyyyyyyyyy iiiiiiiiiiiiiiiiiiii sssayyyyyyyyyyyyyyyy iiiayyyayyyyyyyayyyy\n",
            "Epoch:   1 | Train loss: 2.499 | Val loss: 2.359 | Gen: eayyyyyyyyyyyyyyyyyy ayyyyyyyyyyyyyyyyyyy innnnnnnnnnnnnnnnnnn iayyyyyyyyyyyyyyyyyy iayyyyyyyyyyyyyyyyyy\n",
            "Epoch:   2 | Train loss: 2.312 | Val loss: 2.211 | Gen: othha ayyyyyyyyyyyyyyyyyyy iiiiiiiiiiiiiiiiiiii isayyyyyyyyyyyyyyyyy iiigayyyyyyyyyyyyyyy\n",
            "Epoch:   3 | Train loss: 2.218 | Val loss: 2.141 | Gen: othhhhhhhhha ayyyyyyyyyyyyyyyyyyy innnnnnnnnnnnnnnnnnn osayyyyyyyyyyyyyyyyy inga\n",
            "Epoch:   4 | Train loss: 2.147 | Val loss: 2.080 | Gen: othhha ayyyyyyyyyyyyyyyyyyy innnnnnnnnnnnnnnnnnn osayyyyyyyyyyyyyyyyy iga\n",
            "Epoch:   5 | Train loss: 2.074 | Val loss: 2.029 | Gen: othhay ayyyyyyyyyyyyyyyyyyy innnnnnnnnnnnnnnnnnn osay innnnnnnnnnnnnnnnnnn\n",
            "Epoch:   6 | Train loss: 1.989 | Val loss: 1.949 | Gen: othhhhay ay innnnnnnnnnnnnnnnnnn osay innnnnnnnnnnnnnnnnnn\n",
            "Epoch:   7 | Train loss: 1.905 | Val loss: 1.899 | Gen: ethay ayra innnnnnnnnnnnnnnsnnn osay innnnnngay\n",
            "Epoch:   8 | Train loss: 1.830 | Val loss: 1.837 | Gen: ethay aroray innnnnnnnnnnnnnnnnnn osay innnngay\n",
            "Epoch:   9 | Train loss: 1.782 | Val loss: 1.781 | Gen: ethay arinay innnnnnnnnnnnnnnnnnn isay innnnnnngay\n",
            "Epoch:  10 | Train loss: 1.755 | Val loss: 1.776 | Gen: ethay arinay innnnnnnnnnnnnnnnnnn isay ingngay\n",
            "Epoch:  11 | Train loss: 1.717 | Val loss: 1.716 | Gen: ethay arinay innnnnnnnnnnnnnnnnnn isay ingngay\n",
            "Epoch:  12 | Train loss: 1.649 | Val loss: 1.684 | Gen: ethay airnay innnnnnnnnnnnnnnnnnn isay ingggggggay\n",
            "Epoch:  13 | Train loss: 1.620 | Val loss: 1.635 | Gen: ethay airnay innnnnnnnnnnnnnnnnnn isay ingray\n",
            "Epoch:  14 | Train loss: 1.603 | Val loss: 1.647 | Gen: ethay airway innnnnnnnnnnnnnnnnnn isay igrggay\n",
            "Epoch:  15 | Train loss: 1.568 | Val loss: 1.596 | Gen: ethay airway innnnnnnnnnnnnnntnnn isay ingray\n",
            "Epoch:  16 | Train loss: 1.554 | Val loss: 1.577 | Gen: ethay airway innnnnnnnnnnnnnnnnnn isay ingray\n",
            "Epoch:  17 | Train loss: 1.500 | Val loss: 1.542 | Gen: ethay ariway innnnnnnnnnnnnnnnnnn isay ongeray\n",
            "Epoch:  18 | Train loss: 1.493 | Val loss: 1.523 | Gen: ethay aray innnnnnnnnnnnnnnnnnn isway egerngay\n",
            "Epoch:  19 | Train loss: 1.496 | Val loss: 1.561 | Gen: ethay ariway innnnnnnnnnnnnnnnnnn isway ingray\n",
            "Epoch:  20 | Train loss: 1.446 | Val loss: 1.489 | Gen: ethay ariway innnnnnnnnnnnnnnnnnn isway ingray\n",
            "Epoch:  21 | Train loss: 1.418 | Val loss: 1.461 | Gen: ethay ariway innnnnnnnnnnntnnnnnn isway ingreay\n",
            "Epoch:  22 | Train loss: 1.401 | Val loss: 1.428 | Gen: ethay ariway onnnnnnnnnnnnnntnnnn isway ongeray\n",
            "Epoch:  23 | Train loss: 1.371 | Val loss: 1.410 | Gen: ethay airinay onnnnnnnnnnnnnntay isway onkergay\n",
            "Epoch:  24 | Train loss: 1.353 | Val loss: 1.416 | Gen: ethay airway innnnnnnnnnnnnntnnay isway ongray\n",
            "Epoch:  25 | Train loss: 1.338 | Val loss: 1.375 | Gen: ethay airinway onnnnnnnnnnnntnnnnnn isway onkegray\n",
            "Epoch:  26 | Train loss: 1.296 | Val loss: 1.368 | Gen: ethay airinway innnnnnnnntonnnnnnnn isway onegreday\n",
            "Epoch:  27 | Train loss: 1.284 | Val loss: 1.361 | Gen: ethay airingay innnnnnnnnnnnnnnntnn isway ongreay\n",
            "Epoch:  28 | Train loss: 1.270 | Val loss: 1.348 | Gen: ethay airiway onnnnnnntinnnnnway isay onkegray\n",
            "Epoch:  29 | Train loss: 1.230 | Val loss: 1.304 | Gen: ethay airiway onnnnnnntinnnnncay isway onkreay\n",
            "Epoch:  30 | Train loss: 1.240 | Val loss: 1.315 | Gen: ethay ariway innnnnnnnnnnnnninnnn isway ongreway\n",
            "Epoch:  31 | Train loss: 1.212 | Val loss: 1.302 | Gen: ethay airway innnnnnnnnnntinnnnnn isway onkray\n",
            "Epoch:  32 | Train loss: 1.187 | Val loss: 1.268 | Gen: ethay airway onnnnnnninnnnnnninnt isay orengkay\n",
            "Epoch:  33 | Train loss: 1.157 | Val loss: 1.271 | Gen: ethay ariway onnnnnnnintay isay onkreay\n",
            "Epoch:  34 | Train loss: 1.137 | Val loss: 1.235 | Gen: ethay airway onnnnninntincay isay orengkay\n",
            "Epoch:  35 | Train loss: 1.136 | Val loss: 1.237 | Gen: ethay airway onnnnnningay isinay orengway\n",
            "Epoch:  36 | Train loss: 1.131 | Val loss: 1.239 | Gen: ethay airway onnnnnanntay isiway orngay\n",
            "Epoch:  37 | Train loss: 1.143 | Val loss: 1.278 | Gen: ethay airway onnnnnninnnnntigay isinay onkreay\n",
            "Epoch:  38 | Train loss: 1.121 | Val loss: 1.207 | Gen: ethay airway innnninnntinncay isay orngeway\n",
            "Epoch:  39 | Train loss: 1.077 | Val loss: 1.217 | Gen: ethay airway onnnninnntinway isinay orngay\n",
            "Epoch:  40 | Train loss: 1.084 | Val loss: 1.184 | Gen: ethay airway onnninnntincay isay orngway\n",
            "Epoch:  41 | Train loss: 1.076 | Val loss: 1.189 | Gen: ethay airway innnninnntinnway isinay ongrenway\n",
            "Epoch:  42 | Train loss: 1.047 | Val loss: 1.168 | Gen: ethay airway innnntinncay isiway ukringway\n",
            "Epoch:  43 | Train loss: 1.030 | Val loss: 1.168 | Gen: ethay airway onnninnntincay isiway ongkirway\n",
            "Epoch:  44 | Train loss: 1.037 | Val loss: 1.174 | Gen: ethay airway onnninnntinway isiway ongernway\n",
            "Epoch:  45 | Train loss: 1.014 | Val loss: 1.160 | Gen: ethay airway onnninntincay isiway orngencay\n",
            "Epoch:  46 | Train loss: 1.018 | Val loss: 1.162 | Gen: ethay airway onnninntay isinay orngencay\n",
            "Epoch:  47 | Train loss: 1.014 | Val loss: 1.166 | Gen: ethay airway innnnintincay isiway ongreway\n",
            "Epoch:  48 | Train loss: 1.033 | Val loss: 1.135 | Gen: ethay airway onnninntinway isinay orngway\n",
            "Epoch:  49 | Train loss: 0.985 | Val loss: 1.113 | Gen: ethay airway onnninntay isinay orngeway\n",
            "Epoch:  50 | Train loss: 0.960 | Val loss: 1.118 | Gen: ethay airway onnnintingay isiway orngengway\n",
            "Epoch:  51 | Train loss: 0.978 | Val loss: 1.089 | Gen: ethay airway onnninntincay isinay orngeway\n",
            "Epoch:  52 | Train loss: 0.969 | Val loss: 1.085 | Gen: ethay airway onnninnteay isiway orngeway\n",
            "Epoch:  53 | Train loss: 0.934 | Val loss: 1.069 | Gen: ethay airway onnnintingay isiway orngeway\n",
            "Epoch:  54 | Train loss: 0.939 | Val loss: 1.080 | Gen: ethay airway onnnintineway isiway ukrengway\n",
            "Epoch:  55 | Train loss: 0.933 | Val loss: 1.079 | Gen: ethay airway onnninntingay isiway orngeway\n",
            "Epoch:  56 | Train loss: 0.936 | Val loss: 1.106 | Gen: ethay airway onninntingay isiday okringway\n",
            "Epoch:  57 | Train loss: 0.951 | Val loss: 1.066 | Gen: ethay airway onnnintingay isiway orngway\n",
            "Epoch:  58 | Train loss: 0.911 | Val loss: 1.043 | Gen: ethay airway onnnintincay isiway ognrway\n",
            "Epoch:  59 | Train loss: 0.917 | Val loss: 1.056 | Gen: ethay airway onnninntingay isinway orngeway\n",
            "Epoch:  60 | Train loss: 0.920 | Val loss: 1.028 | Gen: ethay airway onninntingay isiway ogkinrway\n",
            "Epoch:  61 | Train loss: 0.916 | Val loss: 1.047 | Gen: ethay airway onninninteday isiday uknerngway\n",
            "Epoch:  62 | Train loss: 0.898 | Val loss: 1.032 | Gen: ethay airway onninntincay isway uknerway\n",
            "Epoch:  63 | Train loss: 0.887 | Val loss: 1.021 | Gen: ethay airway onninntingay isinway unkerngway\n",
            "Epoch:  64 | Train loss: 0.885 | Val loss: 1.034 | Gen: ethay airway onninntingay isway uknerngway\n",
            "Epoch:  65 | Train loss: 0.886 | Val loss: 1.010 | Gen: ethay airway onninntingay isway orngkigway\n",
            "Epoch:  66 | Train loss: 0.868 | Val loss: 1.011 | Gen: ethay airway onninntingay isiway uknerngway\n",
            "Epoch:  67 | Train loss: 0.879 | Val loss: 1.016 | Gen: ethay airway onninninteday isinway uknerngway\n",
            "Epoch:  68 | Train loss: 0.868 | Val loss: 1.002 | Gen: ethay airway onninntingay isway uknerngway\n",
            "Epoch:  69 | Train loss: 0.834 | Val loss: 0.991 | Gen: ethay airway onninntingay isway orngkigway\n",
            "Epoch:  70 | Train loss: 0.828 | Val loss: 1.021 | Gen: ethay airway onnnintingeday isiwsay orngkigway\n",
            "Epoch:  71 | Train loss: 0.833 | Val loss: 0.984 | Gen: ethay airway onninntingay isway orngway\n",
            "Epoch:  72 | Train loss: 0.840 | Val loss: 0.967 | Gen: ethay airway onninntigeday isiway ukengrway\n",
            "Epoch:  73 | Train loss: 0.799 | Val loss: 0.974 | Gen: ethay airway onninntingay isiway orngkigway\n",
            "Epoch:  74 | Train loss: 0.844 | Val loss: 0.985 | Gen: ethay airway onninntigay isway okringway\n",
            "Epoch:  75 | Train loss: 0.820 | Val loss: 0.956 | Gen: ethay airway onninntingay isway orngkigway\n",
            "Epoch:  76 | Train loss: 0.813 | Val loss: 0.968 | Gen: ethay airway onnintingeday isway uknerway\n",
            "Epoch:  77 | Train loss: 0.805 | Val loss: 0.963 | Gen: ethay airway onnintingeday isway uknerngway\n",
            "Epoch:  78 | Train loss: 0.780 | Val loss: 0.953 | Gen: ethay airway onnintingay isway ukngrengway\n",
            "Epoch:  79 | Train loss: 0.789 | Val loss: 0.956 | Gen: ethay airway onnintinedcay isway ukngrengway\n",
            "Epoch:  80 | Train loss: 0.767 | Val loss: 0.945 | Gen: ethay airway onninvinteway isway uknerwshay\n",
            "Epoch:  81 | Train loss: 0.779 | Val loss: 0.944 | Gen: ethay airway onninntiveway isway uknerway\n",
            "Epoch:  82 | Train loss: 0.762 | Val loss: 0.962 | Gen: ethay airway onninvineday isway uknerngway\n",
            "Epoch:  83 | Train loss: 0.776 | Val loss: 0.953 | Gen: ethay airway onninntingay isway uknerwshay\n",
            "Epoch:  84 | Train loss: 0.770 | Val loss: 0.963 | Gen: ethay airway onninntivenway isway uknerwshay\n",
            "Epoch:  85 | Train loss: 0.769 | Val loss: 0.936 | Gen: ethay airway onninvintincay isway ukngrengway\n",
            "Epoch:  86 | Train loss: 0.762 | Val loss: 0.967 | Gen: ethay airway onninvinteway isway ukngrengway\n",
            "Epoch:  87 | Train loss: 0.782 | Val loss: 0.930 | Gen: ethay airway onninvintidcay isway ukngrengway\n",
            "Epoch:  88 | Train loss: 0.751 | Val loss: 0.942 | Gen: ethay airway onninvinteday isiway uknerwshay\n",
            "Epoch:  89 | Train loss: 0.740 | Val loss: 0.932 | Gen: ethay airway onninvintidcay isway uknerway\n",
            "Epoch:  90 | Train loss: 0.748 | Val loss: 0.939 | Gen: ethay airway onninvinteway isway uknerwshay\n",
            "Epoch:  91 | Train loss: 0.730 | Val loss: 0.901 | Gen: ethay airway onninvintincay isway uknerway\n",
            "Epoch:  92 | Train loss: 0.722 | Val loss: 0.922 | Gen: ethay airway onninvintidcay isiway uknerwshay\n",
            "Epoch:  93 | Train loss: 0.711 | Val loss: 0.909 | Gen: ethay airway onninvintidcay isway uknerwshay\n",
            "Epoch:  94 | Train loss: 0.752 | Val loss: 0.930 | Gen: ethay ariway onninvineday isway orngkigway\n",
            "Epoch:  95 | Train loss: 0.753 | Val loss: 0.915 | Gen: ethay airway onninntingay isway uknerwsay\n",
            "Epoch:  96 | Train loss: 0.733 | Val loss: 0.907 | Gen: ethay airway onninvintidcay isway uknerwshay\n",
            "Epoch:  97 | Train loss: 0.701 | Val loss: 0.917 | Gen: ethay airway onninvintidcay isway uknerwshay\n",
            "Epoch:  98 | Train loss: 0.704 | Val loss: 0.919 | Gen: ethay airway onninvintidcay isway orngkigway\n",
            "Epoch:  99 | Train loss: 0.740 | Val loss: 0.888 | Gen: ethay airway onninvintidcay isway uknerway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onninvintidcay isway uknerway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE-hKCxhF3iR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6ac05fde-45af-4d56-e7fe-a47bfa6ef5db"
      },
      "source": [
        "TEST_SENTENCE = 'deodorant'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tdeodorant \n",
            "translated:\toodoveabreday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8FaZZUWRpY9",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik5rx9qw9KCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULCMHm5ZF7vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbfZCByITOI6",
        "colab_type": "text"
      },
      "source": [
        "# Attention visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itCGMv3FdXsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_WORD_ATTN = 'street'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBv4QQuBiU-V",
        "colab_type": "text"
      },
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXvqoQYONMTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuOvxfA1NMz3",
        "colab_type": "text"
      },
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSSB4wd8-M7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}