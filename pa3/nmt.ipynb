{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "outputId": "9dbf3feb-5763-42b5-b5d2-2533895e1509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python2.7/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torch) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.12.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "  Using cached https://files.pythonhosted.org/packages/b6/4b/5adc1109908266554fb978154c797c7d71aba43dd15508d8c1565648f6bc/Pillow-6.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-6.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.14.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.3.0 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 6.0.0\n",
            "    Uninstalling Pillow-6.0.0:\n",
            "      Successfully uninstalled Pillow-6.0.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    elif opts.decoder_type == 'transformer_non_causal':\n",
        "        decoder = TransformerDecoderNonCausal(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXNsLNkOn38w",
        "colab_type": "text"
      },
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BAfi_8yWB3y",
        "colab_type": "text"
      },
      "source": [
        "## GRU cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ztmyA5Ro67o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        g = torch.tanh(self.Wih(x) + r*self.Whh(h_prev))\n",
        "        h_new = (1-z)*g+z*h_prev\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JBVFLEZWNC1",
        "colab_type": "text"
      },
      "source": [
        "### GRU encoder / decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaDt7XDmWRzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWe0RO5FWajD",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GUK5A7CWhV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1) # changed from dim=1 to dim=2\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = queries.size()[0]\n",
        "        seq_len = keys.size()[1]\n",
        "        expanded_queries = queries.unsqueeze(1).expand_as(keys) # batch_size x seq_len x hidden_size\n",
        "        concat_inputs = torch.cat((expanded_queries, keys), 2) # batch_size x seq_len x 2*hidden_size\n",
        "        unnormalized_attention = self.attention_network(concat_inputs) # batch_size x seq_len x hidden_size\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(1,2), values)\n",
        "        return context, attention_weights\n",
        "        \n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=2) # changed from dim=1 to dim=2\n",
        "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = queries.size()[0]\n",
        "        seq_len = keys.size()[1]\n",
        "        if len(queries.size()) == 2:\n",
        "          queries = queries.unsqueeze(1)\n",
        "          \n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        \n",
        "        unnormalized_attention = torch.bmm(q, k.transpose(1,2))*self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights, v)\n",
        "        return context, attention_weights.transpose(1,2)\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=2) # changed from dim=1 to dim=2\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = queries.size()[0]\n",
        "        seq_len = keys.size()[1]\n",
        "        if len(queries.size()) == 2:\n",
        "          queries = queries.unsqueeze(1)\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(q, k.transpose(1,2))*self.scaling_factor # batch_size x k x seq_len\n",
        "        mask = self.neg_inf*torch.ones_like(unnormalized_attention).triu(1) # only keep lower diagonal of unnormalized_attention\n",
        "        attention_weights = self.softmax(unnormalized_attention + mask) # batch_size x k x seq_len\n",
        "        context = torch.bmm(attention_weights, v) # batch_size x k x hidden_size\n",
        "        return context, attention_weights.transpose(1,2)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pemjZo2XWtRt",
        "colab_type": "text"
      },
      "source": [
        "### Attention decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfjF0Z-PWwPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN\n",
        "            # ------------\n",
        "            embed_current = embed[:, i, :] # batch_size x hidden_size\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations) # context = batch_size x 1 x hidden_size\n",
        "            context = context.squeeze(1) # batch_size x hidden_size\n",
        "            embed_and_context = torch.cat((embed_current, context), 1)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)\n",
        "\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8JpcwTRW5cw",
        "colab_type": "text"
      },
      "source": [
        "### Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5vJPku1W7sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN\n",
        "          # ------------\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
        "          residual_contexts = contexts + new_contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = residual_contexts + new_contexts\n",
        "\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfp38gBDETcJ",
        "colab_type": "text"
      },
      "source": [
        "### Transformer Non Causal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ID4vxiqEWnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoderNonCausal(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoderNonCausal, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN\n",
        "          # ------------\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
        "          residual_contexts = contexts + new_contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = residual_contexts + new_contexts\n",
        "\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNFd6LNo0-o",
        "colab_type": "text"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiUwiOITHTW4",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwcFjsEpHRbI",
        "colab_type": "code",
        "outputId": "a8a89357-9613-4455-e5cd-322dc514052c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQmyJDSRFKR",
        "colab_type": "text"
      },
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2281
        },
        "outputId": "dc722130-befb-49f5-82be-d2b8ef9e82a6"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNNDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.408 | Val loss: 2.031 | Gen: esteray antay eray estay eray\n",
            "Epoch:   1 | Train loss: 1.940 | Val loss: 1.849 | Gen: eray antay onteray eray oonteray\n",
            "Epoch:   2 | Train loss: 1.789 | Val loss: 1.773 | Gen: ersay aray ontersay ersay oonteray\n",
            "Epoch:   3 | Train loss: 1.686 | Val loss: 1.686 | Gen: estay aray ontertay ingsay oousterday\n",
            "Epoch:   4 | Train loss: 1.609 | Val loss: 1.643 | Gen: estay aringway onterteray ighedway oousterday\n",
            "Epoch:   5 | Train loss: 1.548 | Val loss: 1.593 | Gen: estay aringway ontersenstay ighedway oousterday\n",
            "Epoch:   6 | Train loss: 1.490 | Val loss: 1.556 | Gen: estay arighway onterstay ightray ooustray\n",
            "Epoch:   7 | Train loss: 1.453 | Val loss: 1.561 | Gen: eay arighway intingshay ightray oordway\n",
            "Epoch:   8 | Train loss: 1.417 | Val loss: 1.492 | Gen: estay arighway ondingway ighway ourtray\n",
            "Epoch:   9 | Train loss: 1.375 | Val loss: 1.466 | Gen: estay arighway ondingway ighway ourngray\n",
            "Epoch:  10 | Train loss: 1.346 | Val loss: 1.441 | Gen: estay away-awlay ondingray ighshay ourdway\n",
            "Epoch:  11 | Train loss: 1.314 | Val loss: 1.422 | Gen: estay arighway ondingray ighshay ourdway\n",
            "Epoch:  12 | Train loss: 1.287 | Val loss: 1.403 | Gen: estay arighway intray ighway ourngray\n",
            "Epoch:  13 | Train loss: 1.261 | Val loss: 1.390 | Gen: estay arighay intray-onsway ighlay ourngray\n",
            "Epoch:  14 | Train loss: 1.240 | Val loss: 1.364 | Gen: edway arighway intray-onday ighlyway ourngray\n",
            "Epoch:  15 | Train loss: 1.217 | Val loss: 1.404 | Gen: eway arighway ondingway ighlyway ourghtnay\n",
            "Epoch:  16 | Train loss: 1.209 | Val loss: 1.348 | Gen: ethay arighlay ondingray ighlyway oughtray\n",
            "Epoch:  17 | Train loss: 1.171 | Val loss: 1.335 | Gen: ethay arighway ondingray ighstay oughtray\n",
            "Epoch:  18 | Train loss: 1.163 | Val loss: 1.305 | Gen: ethay aingway ondingray ighlyway oughtray\n",
            "Epoch:  19 | Train loss: 1.148 | Val loss: 1.330 | Gen: eedhay aingway intoringway issshay oughtray\n",
            "Epoch:  20 | Train loss: 1.154 | Val loss: 1.323 | Gen: estay aingway ondingray ighlay oughtray\n",
            "Epoch:  21 | Train loss: 1.118 | Val loss: 1.277 | Gen: ethay aingway ondingray-onday isshay oughtray\n",
            "Epoch:  22 | Train loss: 1.090 | Val loss: 1.266 | Gen: ethay aingway ondingray-onday issway oughtray\n",
            "Epoch:  23 | Train loss: 1.075 | Val loss: 1.279 | Gen: ethay airyway ondingray-oornay ishway oughtray\n",
            "Epoch:  24 | Train loss: 1.063 | Val loss: 1.240 | Gen: ethay aingway ondingray issway oughtray\n",
            "Epoch:  25 | Train loss: 1.060 | Val loss: 1.251 | Gen: ethay airyray ondingray issway oughtray\n",
            "Epoch:  26 | Train loss: 1.042 | Val loss: 1.240 | Gen: ethay airyray ondintingway issway oughtray\n",
            "Epoch:  27 | Train loss: 1.041 | Val loss: 1.237 | Gen: ethay ayigray ondintingway issway ourghtray\n",
            "Epoch:  28 | Train loss: 1.024 | Val loss: 1.202 | Gen: ethay airyray ontingray-offay issway oughtray\n",
            "Epoch:  29 | Train loss: 1.015 | Val loss: 1.197 | Gen: ethay airyray onduntsingway issway oughtray\n",
            "Epoch:  30 | Train loss: 1.002 | Val loss: 1.214 | Gen: ethay airyray ontingray-oringway issway oughtray\n",
            "Epoch:  31 | Train loss: 0.994 | Val loss: 1.183 | Gen: ethay airyray ondurentway isway ourghtray\n",
            "Epoch:  32 | Train loss: 0.980 | Val loss: 1.166 | Gen: ethay airyray ondundingtay isway ourghtray\n",
            "Epoch:  33 | Train loss: 0.965 | Val loss: 1.167 | Gen: ethay airyray ondundingway issway ourghtray\n",
            "Epoch:  34 | Train loss: 0.966 | Val loss: 1.152 | Gen: ethay airyray ondundway issway ourghlay\n",
            "Epoch:  35 | Train loss: 0.946 | Val loss: 1.149 | Gen: ethay airyray ondundway isway ourghlay\n",
            "Epoch:  36 | Train loss: 0.944 | Val loss: 1.144 | Gen: ethay airyray onduntsay issway ourghtray\n",
            "Epoch:  37 | Train loss: 0.934 | Val loss: 1.172 | Gen: ethay airyray ondurencedway issway oughtnay\n",
            "Epoch:  38 | Train loss: 0.933 | Val loss: 1.124 | Gen: ethay airyway onfingray-oodray isway ourgray\n",
            "Epoch:  39 | Train loss: 0.928 | Val loss: 1.199 | Gen: ethay airyray ondurcencay issway ourgray\n",
            "Epoch:  40 | Train loss: 0.930 | Val loss: 1.124 | Gen: ethay airyray onfingray-ingway isway ourgray\n",
            "Epoch:  41 | Train loss: 0.909 | Val loss: 1.119 | Gen: ethay airyray ondunsiceway issway ourgray\n",
            "Epoch:  42 | Train loss: 0.889 | Val loss: 1.106 | Gen: ethay airyway onfindingray isway ourgray\n",
            "Epoch:  43 | Train loss: 0.879 | Val loss: 1.099 | Gen: ethay airyway ondundway isway ourgray\n",
            "Epoch:  44 | Train loss: 0.877 | Val loss: 1.088 | Gen: ethay airyway ondunsinglay isway orglinglay\n",
            "Epoch:  45 | Train loss: 0.870 | Val loss: 1.105 | Gen: eedhay airyway onduntarcay issway orkingway\n",
            "Epoch:  46 | Train loss: 0.862 | Val loss: 1.098 | Gen: ethay airyway ondunsiceway issway orglingpay\n",
            "Epoch:  47 | Train loss: 0.861 | Val loss: 1.078 | Gen: ethay airyway onfingray-oringway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.850 | Val loss: 1.084 | Gen: ethay airyway ondurencylay issway orglicyvay\n",
            "Epoch:  49 | Train loss: 0.864 | Val loss: 1.068 | Gen: ethay airyway onduntandway isway orkingway\n",
            "Epoch:  50 | Train loss: 0.840 | Val loss: 1.044 | Gen: ethay airyway ondunsiceway ispray orglingay\n",
            "Epoch:  51 | Train loss: 0.824 | Val loss: 1.051 | Gen: ethay airyway ondunsiceway isway orglingway\n",
            "Epoch:  52 | Train loss: 0.826 | Val loss: 1.068 | Gen: ethay airyway onfindway-ingtay isway orglingay\n",
            "Epoch:  53 | Train loss: 0.822 | Val loss: 1.050 | Gen: ethay airyway ondunsiceway ispray orglingay\n",
            "Epoch:  54 | Train loss: 0.823 | Val loss: 1.046 | Gen: ethay airyway ondunsiceway isway orglingpay\n",
            "Epoch:  55 | Train loss: 0.800 | Val loss: 1.030 | Gen: ethay airyway ondunsiceway isway orglingay\n",
            "Epoch:  56 | Train loss: 0.786 | Val loss: 1.030 | Gen: ethay airyway ondunsiceway isway orglingay\n",
            "Epoch:  57 | Train loss: 0.821 | Val loss: 1.078 | Gen: ethay airyway ondurenfay isway orglingay\n",
            "Epoch:  58 | Train loss: 0.824 | Val loss: 1.023 | Gen: ethay airyway ondunsiceway isway orglingcay\n",
            "Epoch:  59 | Train loss: 0.793 | Val loss: 1.023 | Gen: ethay airyway ondunsiceway isway orglingway\n",
            "Epoch:  60 | Train loss: 0.782 | Val loss: 1.017 | Gen: ethay airyway ondunsityway isway orglioncay\n",
            "Epoch:  61 | Train loss: 0.774 | Val loss: 1.020 | Gen: ethay airyway ondunsiceway ispray orglingway\n",
            "Epoch:  62 | Train loss: 0.764 | Val loss: 1.002 | Gen: ethay airyway ondunsiceway isway orglingay\n",
            "Epoch:  63 | Train loss: 0.766 | Val loss: 0.993 | Gen: ethay airyway ondurencylay isway orglioncay\n",
            "Epoch:  64 | Train loss: 0.749 | Val loss: 0.998 | Gen: ethay airyway ondunsityway isway orglingay\n",
            "Epoch:  65 | Train loss: 0.743 | Val loss: 0.997 | Gen: ethay airyway ondunsiceway isway orglioncay\n",
            "Epoch:  66 | Train loss: 0.737 | Val loss: 0.987 | Gen: ethay airyway ondunsityway isway orglionway\n",
            "Epoch:  67 | Train loss: 0.756 | Val loss: 1.056 | Gen: ethay airyway ondurenfay ispray orglionway\n",
            "Epoch:  68 | Train loss: 0.814 | Val loss: 1.015 | Gen: ethay airyway onfustingfay isway orinkway\n",
            "Epoch:  69 | Train loss: 0.770 | Val loss: 1.016 | Gen: ethay airyway ondunsiceway isway orglioncay\n",
            "Epoch:  70 | Train loss: 0.738 | Val loss: 0.968 | Gen: ethay airyway ondunsityway isway orglioncay\n",
            "Epoch:  71 | Train loss: 0.721 | Val loss: 0.972 | Gen: ethay airyway ondunsityway isway orglioncay\n",
            "Epoch:  72 | Train loss: 0.713 | Val loss: 0.963 | Gen: ethay airyway ondunsityway isway orglioncay\n",
            "Epoch:  73 | Train loss: 0.719 | Val loss: 0.976 | Gen: ethay airyway ondunsityway isway orglioncay\n",
            "Epoch:  74 | Train loss: 0.712 | Val loss: 0.964 | Gen: ethay airyway ondunsityway isway orglionway\n",
            "Epoch:  75 | Train loss: 0.709 | Val loss: 0.962 | Gen: ethay airway ondurencylay isway orglioncay\n",
            "Epoch:  76 | Train loss: 0.700 | Val loss: 0.967 | Gen: ethay airway ondunsityway isway orglionway\n",
            "Epoch:  77 | Train loss: 0.704 | Val loss: 0.972 | Gen: ethay airway ondunsityway isway orgingolay\n",
            "Epoch:  78 | Train loss: 0.710 | Val loss: 0.967 | Gen: ethay airyway ondunsiceway isway orglioncay\n",
            "Epoch:  79 | Train loss: 0.710 | Val loss: 0.955 | Gen: ethay airway ondunsityway isway orglionway\n",
            "Epoch:  80 | Train loss: 0.696 | Val loss: 0.942 | Gen: ethay airway onductionsay isway orglioncay\n",
            "Epoch:  81 | Train loss: 0.685 | Val loss: 0.941 | Gen: ethay airway ondunsityway isway orginkway\n",
            "Epoch:  82 | Train loss: 0.679 | Val loss: 0.952 | Gen: ethay airway onductionsay isway orglioncay\n",
            "Epoch:  83 | Train loss: 0.675 | Val loss: 0.944 | Gen: ethay airway ondunsityway isway orglionway\n",
            "Epoch:  84 | Train loss: 0.695 | Val loss: 1.021 | Gen: ethay airway ondurentclay isway orglionway\n",
            "Epoch:  85 | Train loss: 0.690 | Val loss: 0.934 | Gen: ethay airway ondunedlay isway orginkway\n",
            "Epoch:  86 | Train loss: 0.695 | Val loss: 0.951 | Gen: ethay airway ondurentcray isway orginkway\n",
            "Epoch:  87 | Train loss: 0.691 | Val loss: 0.940 | Gen: ethay airyway ondunedlay isway orinkway\n",
            "Epoch:  88 | Train loss: 0.666 | Val loss: 0.928 | Gen: ethay airway ondunsityway isway orginkway\n",
            "Epoch:  89 | Train loss: 0.660 | Val loss: 0.935 | Gen: ethay airway ondunedlay isway orginkway\n",
            "Epoch:  90 | Train loss: 0.647 | Val loss: 0.931 | Gen: ethay airway ondunedlay isway orginkway\n",
            "Epoch:  91 | Train loss: 0.648 | Val loss: 0.937 | Gen: ethay airway ondurentcypay isway orginkway\n",
            "Epoch:  92 | Train loss: 0.644 | Val loss: 0.933 | Gen: ethay airway ondurentcypay isway orglionway\n",
            "Epoch:  93 | Train loss: 0.746 | Val loss: 1.230 | Gen: estay airyway onlfingsway isway orglionway\n",
            "Epoch:  94 | Train loss: 0.774 | Val loss: 0.948 | Gen: ethay airyway ondunityway isway orgionpay\n",
            "Epoch:  95 | Train loss: 0.668 | Val loss: 0.925 | Gen: ethay airyway ondunedlay isway orinkway\n",
            "Epoch:  96 | Train loss: 0.646 | Val loss: 0.924 | Gen: ethay airyway ondunedlay-inway isway orikencay\n",
            "Epoch:  97 | Train loss: 0.637 | Val loss: 0.918 | Gen: ethay airway ondunedlay isway orinkway\n",
            "Epoch:  98 | Train loss: 0.630 | Val loss: 0.924 | Gen: ethay airway ondunedlay isway orginkway\n",
            "Epoch:  99 | Train loss: 0.630 | Val loss: 0.921 | Gen: ethay airway onductionsmay isway orginkway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onductionsmay isway orginkway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2kPGj5DFv7a",
        "colab_type": "code",
        "outputId": "c0f9a9b6-3974-44b3-9254-81764b099554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "\n",
        "TEST_SENTENCE = 'this string should be relatively difficult to explain to anthropologists'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onductionsmay isway orginkway\n",
            "source:\t\tthis string should be relatively difficult to explain to anthropologists \n",
            "translated:\tisthay ingstay uldhay ebay elabletiglyway iffucuredlay otay explackway otay anthtlay-ossionway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cP7nl5NRJbu",
        "colab_type": "text"
      },
      "source": [
        "## RNN attention decoder Additive Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKlyfbuPDXDR",
        "colab_type": "code",
        "outputId": "a3bfd02c-f09e-4871-f650-6872a0983ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2315
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AdditiveAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.393 | Val loss: 1.974 | Gen: eray ay enenensay eray enseray\n",
            "Epoch:   1 | Train loss: 1.874 | Val loss: 1.774 | Gen: eray aray ongtingtay eredway ongtingtay\n",
            "Epoch:   2 | Train loss: 1.663 | Val loss: 1.603 | Gen: estay aray ongingtingay isedway oongstay\n",
            "Epoch:   3 | Train loss: 1.482 | Val loss: 1.416 | Gen: estay aray ongongay isssay ooringway\n",
            "Epoch:   4 | Train loss: 1.285 | Val loss: 1.211 | Gen: etray ariday onghingay issay oringindway\n",
            "Epoch:   5 | Train loss: 1.089 | Val loss: 1.012 | Gen: ehtway airyway ongingay issay oringray\n",
            "Epoch:   6 | Train loss: 0.897 | Val loss: 0.831 | Gen: eaththay airway onditingay issay oriingday\n",
            "Epoch:   7 | Train loss: 0.734 | Val loss: 0.722 | Gen: eethtay airway onditingay isway oringway\n",
            "Epoch:   8 | Train loss: 0.672 | Val loss: 0.689 | Gen: eway aiirway onditiongway issay orkingway\n",
            "Epoch:   9 | Train loss: 0.635 | Val loss: 0.793 | Gen: ehthay airway ondingay-ingray issay orkingway\n",
            "Epoch:  10 | Train loss: 0.562 | Val loss: 0.501 | Gen: eay-othay airway onditiongingway isway oroigway\n",
            "Epoch:  11 | Train loss: 0.435 | Val loss: 0.412 | Gen: ehtay airway onditioningway isway orkingway\n",
            "Epoch:  12 | Train loss: 0.380 | Val loss: 0.471 | Gen: ehtay airway onditiongway isway orkingway\n",
            "Epoch:  13 | Train loss: 0.354 | Val loss: 0.358 | Gen: ehtay airway onditioningway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.356 | Val loss: 0.422 | Gen: ehtay airway onditiongway isway orkingway\n",
            "Epoch:  15 | Train loss: 0.379 | Val loss: 0.393 | Gen: ehtay airway onditiongingway isway orkingway\n",
            "Epoch:  16 | Train loss: 0.267 | Val loss: 0.359 | Gen: elay airway onditiongingway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.214 | Val loss: 0.317 | Gen: elay airway onditiongway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.239 | Val loss: 0.297 | Gen: elay airway onditiongway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.193 | Val loss: 0.308 | Gen: eway airway onditiongingway isway orkingway\n",
            "Epoch:  20 | Train loss: 0.199 | Val loss: 0.264 | Gen: eday airway onditiongay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.169 | Val loss: 0.286 | Gen: eway airway onditiongway isway orkingway\n",
            "Epoch:  22 | Train loss: 0.164 | Val loss: 0.228 | Gen: elay airway onditioningway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.150 | Val loss: 0.218 | Gen: ehay airway onditiongingway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.108 | Val loss: 0.217 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.188 | Val loss: 0.296 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.179 | Val loss: 0.206 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.114 | Val loss: 0.149 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.091 | Val loss: 0.139 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.070 | Val loss: 0.135 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.055 | Val loss: 0.130 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.080 | Val loss: 0.167 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.111 | Val loss: 0.177 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.065 | Val loss: 0.109 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.051 | Val loss: 0.160 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.085 | Val loss: 0.216 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.075 | Val loss: 0.125 | Gen: ethay airway onditionicgcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.040 | Val loss: 0.096 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.032 | Val loss: 0.158 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.035 | Val loss: 0.158 | Gen: ethay airway onditionway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.101 | Val loss: 0.146 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.156 | Val loss: 0.174 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.064 | Val loss: 0.112 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.041 | Val loss: 0.092 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.027 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.021 | Val loss: 0.068 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.017 | Val loss: 0.063 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.016 | Val loss: 0.073 | Gen: ethay airway onditionway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.013 | Val loss: 0.070 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.011 | Val loss: 0.055 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.011 | Val loss: 0.057 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.175 | Val loss: 0.291 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.084 | Val loss: 0.125 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  53 | Train loss: 0.038 | Val loss: 0.070 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.017 | Val loss: 0.066 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.012 | Val loss: 0.067 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.009 | Val loss: 0.060 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.008 | Val loss: 0.065 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.007 | Val loss: 0.070 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.007 | Val loss: 0.062 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.006 | Val loss: 0.058 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.005 | Val loss: 0.056 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.004 | Val loss: 0.055 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.004 | Val loss: 0.056 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.004 | Val loss: 0.050 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.003 | Val loss: 0.049 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.003 | Val loss: 0.046 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.003 | Val loss: 0.044 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.002 | Val loss: 0.043 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.002 | Val loss: 0.041 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.002 | Val loss: 0.040 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.002 | Val loss: 0.039 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.002 | Val loss: 0.038 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.002 | Val loss: 0.037 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.002 | Val loss: 0.036 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.001 | Val loss: 0.036 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.001 | Val loss: 0.035 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.001 | Val loss: 0.035 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.001 | Val loss: 0.035 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.002 | Val loss: 0.101 | Gen: ethay airway onditioncac isway orkingway\n",
            "Epoch:  80 | Train loss: 0.300 | Val loss: 0.289 | Gen: etay airway onditionimndcay isway orkingwdway\n",
            "Epoch:  81 | Train loss: 0.173 | Val loss: 0.154 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.071 | Val loss: 0.105 | Gen: ethay airway onditionicgcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.028 | Val loss: 0.066 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.013 | Val loss: 0.059 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.009 | Val loss: 0.050 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.006 | Val loss: 0.049 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.005 | Val loss: 0.044 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.005 | Val loss: 0.044 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.004 | Val loss: 0.044 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.004 | Val loss: 0.042 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.003 | Val loss: 0.042 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.003 | Val loss: 0.042 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.003 | Val loss: 0.041 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.003 | Val loss: 0.041 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.002 | Val loss: 0.041 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.002 | Val loss: 0.041 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.002 | Val loss: 0.041 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.002 | Val loss: 0.040 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.002 | Val loss: 0.040 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51T3Gpk8orOE",
        "colab_type": "code",
        "outputId": "cde2a998-ff06-4773-f4f6-5af9aa212a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "\n",
        "TEST_SENTENCE = 'this string should be relatively difficult to explain to anthropologists'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n",
            "source:\t\tthis string should be relatively difficult to explain to anthropologists \n",
            "translated:\tisthay ingstray ouldshay ebay elativelyray ifficultday otay explainway otay anthropoogistsway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqgEUU0XleN0",
        "colab_type": "text"
      },
      "source": [
        "## RNN attention decoder Scaled Dot Product Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZGg270GliKX",
        "colab_type": "code",
        "outputId": "765a6669-0187-454a-8a69-08865c30801d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2247
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'scaled_dot', # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder2, rnn_attn_decoder2 = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder2, rnn_attn_decoder2, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: scaled_dot                             \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.370 | Val loss: 1.968 | Gen: eesesay ay oncensay eray oncay\n",
            "Epoch:   1 | Train loss: 1.850 | Val loss: 1.715 | Gen: estay aray ongcay iontay ongcay\n",
            "Epoch:   2 | Train loss: 1.610 | Val loss: 1.544 | Gen: esteay arttay oningsay isttay ongsay\n",
            "Epoch:   3 | Train loss: 1.427 | Val loss: 1.413 | Gen: essay ateray oningingsay istay oungsay\n",
            "Epoch:   4 | Train loss: 1.287 | Val loss: 1.270 | Gen: estay allray ontiningway istay oringsingway\n",
            "Epoch:   5 | Train loss: 1.125 | Val loss: 1.167 | Gen: eaysay airarray ontitingway istay orkingnay\n",
            "Epoch:   6 | Train loss: 1.028 | Val loss: 1.016 | Gen: eway alrray ondticinglay istay orkingway\n",
            "Epoch:   7 | Train loss: 0.867 | Val loss: 0.889 | Gen: eway alrray onditionglay istray orkingway\n",
            "Epoch:   8 | Train loss: 0.800 | Val loss: 0.862 | Gen: eway alrray onditionglay issay orkinghay\n",
            "Epoch:   9 | Train loss: 0.766 | Val loss: 0.875 | Gen: eway airarray onditiongway istay orkingway\n",
            "Epoch:  10 | Train loss: 0.667 | Val loss: 0.682 | Gen: eway airray onditiongnay isstay orkingway\n",
            "Epoch:  11 | Train loss: 0.562 | Val loss: 0.667 | Gen: eway airway onditiongway isway orkingway\n",
            "Epoch:  12 | Train loss: 0.564 | Val loss: 0.623 | Gen: epay airway onditiongway isway orkingway\n",
            "Epoch:  13 | Train loss: 0.501 | Val loss: 0.563 | Gen: eway airway onditiongway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.427 | Val loss: 0.539 | Gen: eway airway onditiongway isway orkingway\n",
            "Epoch:  15 | Train loss: 0.401 | Val loss: 0.514 | Gen: eway airway onditionglay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.406 | Val loss: 0.626 | Gen: epay airway onditiongway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.400 | Val loss: 0.477 | Gen: epay airsay onditiongway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.341 | Val loss: 0.463 | Gen: eway airway onditiongway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.341 | Val loss: 0.418 | Gen: ehay airway onditiongway isway orkingway\n",
            "Epoch:  20 | Train loss: 0.283 | Val loss: 0.383 | Gen: ehay airway onditiongway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.257 | Val loss: 0.388 | Gen: ehay airway onditiongway isway orkingway\n",
            "Epoch:  22 | Train loss: 0.235 | Val loss: 0.344 | Gen: epay airayday onditioningcay isay orkingway\n",
            "Epoch:  23 | Train loss: 0.271 | Val loss: 0.525 | Gen: esay airaysay onditioninglay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.310 | Val loss: 0.389 | Gen: ehay airway onditiongnay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.266 | Val loss: 0.404 | Gen: ehay airway onditiongway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.429 | Val loss: 0.404 | Gen: ehay airway onditioninglay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.235 | Val loss: 0.390 | Gen: ehay airway onditionkway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.200 | Val loss: 0.352 | Gen: ehay airway onditionkway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.159 | Val loss: 0.301 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.135 | Val loss: 0.281 | Gen: ehay airway onditioninglay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.134 | Val loss: 0.285 | Gen: ehay airway onditioningway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.114 | Val loss: 0.269 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.104 | Val loss: 0.275 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.098 | Val loss: 0.266 | Gen: estay airway onditioningway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.102 | Val loss: 0.331 | Gen: estay airway onditioninglay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.121 | Val loss: 0.289 | Gen: ehtay airway onditioninglay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.099 | Val loss: 0.267 | Gen: esthay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.101 | Val loss: 0.311 | Gen: ehay airway onditionkway isway orkingway\n",
            "Epoch:  39 | Train loss: 0.180 | Val loss: 0.324 | Gen: espay airway onditioningnay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.199 | Val loss: 0.318 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.113 | Val loss: 0.220 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.095 | Val loss: 0.212 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.087 | Val loss: 0.227 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.105 | Val loss: 0.251 | Gen: epay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.066 | Val loss: 0.191 | Gen: epay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.059 | Val loss: 0.203 | Gen: epay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.042 | Val loss: 0.164 | Gen: epay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.032 | Val loss: 0.161 | Gen: epay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.028 | Val loss: 0.151 | Gen: epay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.041 | Val loss: 0.206 | Gen: epay airway onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.049 | Val loss: 0.187 | Gen: ehay airway onditioningway isway orkingway\n",
            "Epoch:  52 | Train loss: 0.048 | Val loss: 0.143 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.033 | Val loss: 0.142 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.024 | Val loss: 0.154 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.021 | Val loss: 0.135 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.019 | Val loss: 0.134 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.016 | Val loss: 0.137 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.015 | Val loss: 0.130 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.014 | Val loss: 0.144 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.012 | Val loss: 0.136 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.012 | Val loss: 0.160 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.099 | Val loss: 0.671 | Gen: efay-pay airway onditioninglay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.334 | Val loss: 0.432 | Gen: espay airway onditionftay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.175 | Val loss: 0.291 | Gen: epay airway onditioningcay isatway orkingway\n",
            "Epoch:  65 | Train loss: 0.081 | Val loss: 0.186 | Gen: ehay airway onditioningcay isisiay orkingway\n",
            "Epoch:  66 | Train loss: 0.058 | Val loss: 0.164 | Gen: ethay airway onditioningcay isisiway orkingway\n",
            "Epoch:  67 | Train loss: 0.044 | Val loss: 0.139 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.030 | Val loss: 0.132 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.025 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.019 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.016 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.015 | Val loss: 0.121 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.013 | Val loss: 0.122 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.012 | Val loss: 0.120 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.011 | Val loss: 0.120 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.012 | Val loss: 0.120 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.010 | Val loss: 0.119 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.009 | Val loss: 0.118 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.010 | Val loss: 0.131 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.043 | Val loss: 0.288 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.143 | Val loss: 0.282 | Gen: ethay airway onditionfay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.088 | Val loss: 0.167 | Gen: ehpay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.097 | Val loss: 0.319 | Gen: isthay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.134 | Val loss: 0.252 | Gen: ephay airway onditioncay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.064 | Val loss: 0.134 | Gen: ephay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.030 | Val loss: 0.126 | Gen: ephay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.020 | Val loss: 0.124 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.017 | Val loss: 0.123 | Gen: ephay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.014 | Val loss: 0.125 | Gen: ephay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.012 | Val loss: 0.125 | Gen: ephay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.011 | Val loss: 0.125 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.010 | Val loss: 0.126 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.009 | Val loss: 0.126 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.009 | Val loss: 0.124 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.008 | Val loss: 0.125 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.008 | Val loss: 0.124 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.007 | Val loss: 0.125 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.007 | Val loss: 0.124 | Gen: espay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.006 | Val loss: 0.126 | Gen: espay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tespay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpdpedrGq73-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1ee0bf5e-33e9-43b8-c02e-d0d9bff11df4"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder2, rnn_attn_decoder2, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "\n",
        "TEST_SENTENCE = 'this string should be relatively difficult to explain to anthropologists'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder2, rnn_attn_decoder2, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tespay airway onditioningcay isway orkingway\n",
            "source:\t\tthis string should be relatively difficult to explain to anthropologists \n",
            "translated:\tisthay ingstray ouldshay ebay elativelyray ifficultday otay explainway otay anthropopodtsway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8FaZZUWRpY9",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik5rx9qw9KCg",
        "colab_type": "code",
        "outputId": "246728bf-2896-46a8-df31-f038ae31e77b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2281
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                 num_transformer_layers: 3                                      \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: transformer                            \n",
            "                               lr_decay: 0.99                                   \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TransformerDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CausalScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.168 | Val loss: 1.717 | Gen: edtitay aistay ingiooongiooooongioo issstsssssssssssssss oungay\n",
            "Epoch:   1 | Train loss: 1.557 | Val loss: 1.356 | Gen: ehthehay airairay ongitiongiongiongway ishay ouringringray\n",
            "Epoch:   2 | Train loss: 1.206 | Val loss: 1.076 | Gen: eway airway onditionday ishay oringhinghay\n",
            "Epoch:   3 | Train loss: 0.962 | Val loss: 0.894 | Gen: ethay airway onditiongay isway orkinghay\n",
            "Epoch:   4 | Train loss: 0.790 | Val loss: 0.737 | Gen: ethay airway onditiniongcay isway orkingay\n",
            "Epoch:   5 | Train loss: 0.661 | Val loss: 0.656 | Gen: ethay airway onditiongcay isway orkinghay\n",
            "Epoch:   6 | Train loss: 0.505 | Val loss: 0.547 | Gen: ethay airway ondionionionionionio isway orkingway\n",
            "Epoch:   7 | Train loss: 0.448 | Val loss: 0.490 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:   8 | Train loss: 0.365 | Val loss: 0.464 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.327 | Val loss: 0.397 | Gen: ethay airway ondiningcay isway orkingway\n",
            "Epoch:  10 | Train loss: 0.314 | Val loss: 0.368 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.272 | Val loss: 0.336 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.252 | Val loss: 0.273 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.171 | Val loss: 0.246 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.144 | Val loss: 0.316 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.209 | Val loss: 0.313 | Gen: ethay airway onditininingcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.177 | Val loss: 0.263 | Gen: ethay airway onditinininingcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.261 | Val loss: 0.375 | Gen: ethay airway ondioningcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.247 | Val loss: 0.274 | Gen: ethay airway onditiscay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.169 | Val loss: 0.238 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.123 | Val loss: 0.204 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.090 | Val loss: 0.150 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.103 | Val loss: 0.187 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.081 | Val loss: 0.161 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.060 | Val loss: 0.139 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.052 | Val loss: 0.143 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.124 | Val loss: 0.271 | Gen: ethawhay airway onditiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.116 | Val loss: 0.145 | Gen: ethay airway onditioniongcy isway orkingway\n",
            "Epoch:  28 | Train loss: 0.103 | Val loss: 0.173 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.116 | Val loss: 0.215 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.117 | Val loss: 0.130 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.067 | Val loss: 0.094 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.083 | Val loss: 0.175 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.077 | Val loss: 0.127 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.042 | Val loss: 0.117 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.041 | Val loss: 0.109 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.040 | Val loss: 0.098 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.047 | Val loss: 0.154 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.094 | Val loss: 0.176 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.114 | Val loss: 0.136 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.057 | Val loss: 0.124 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.075 | Val loss: 0.173 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.053 | Val loss: 0.086 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.036 | Val loss: 0.068 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.026 | Val loss: 0.072 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.018 | Val loss: 0.056 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.016 | Val loss: 0.078 | Gen: ethay airway ondititingconinining isway orkingway\n",
            "Epoch:  47 | Train loss: 0.017 | Val loss: 0.063 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.032 | Val loss: 0.093 | Gen: ethay airway onditiongcininingcon isway orkingway\n",
            "Epoch:  49 | Train loss: 0.095 | Val loss: 0.248 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.086 | Val loss: 0.086 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.039 | Val loss: 0.075 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.022 | Val loss: 0.096 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.015 | Val loss: 0.065 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.010 | Val loss: 0.064 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.031 | Val loss: 0.087 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.178 | Val loss: 0.259 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.092 | Val loss: 0.089 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.038 | Val loss: 0.096 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.037 | Val loss: 0.123 | Gen: ethay airway onditionitingcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.020 | Val loss: 0.059 | Gen: ethay airway onditionitay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.010 | Val loss: 0.055 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.006 | Val loss: 0.051 | Gen: ethay airway onditionitay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.006 | Val loss: 0.056 | Gen: ethay airway onditionitway isway orkingway\n",
            "Epoch:  64 | Train loss: 0.007 | Val loss: 0.055 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.008 | Val loss: 0.058 | Gen: ethay airway onditionitingcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.006 | Val loss: 0.057 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.005 | Val loss: 0.064 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.006 | Val loss: 0.062 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.007 | Val loss: 0.072 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.011 | Val loss: 0.062 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.038 | Val loss: 0.127 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.079 | Val loss: 0.203 | Gen: ethay airway ondititiningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.080 | Val loss: 0.109 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.029 | Val loss: 0.071 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.010 | Val loss: 0.060 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.007 | Val loss: 0.060 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.004 | Val loss: 0.059 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.003 | Val loss: 0.057 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.003 | Val loss: 0.062 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.003 | Val loss: 0.060 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.003 | Val loss: 0.063 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.071 | Val loss: 0.223 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.141 | Val loss: 0.146 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.081 | Val loss: 0.175 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.064 | Val loss: 0.065 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.021 | Val loss: 0.057 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.009 | Val loss: 0.051 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.005 | Val loss: 0.050 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.003 | Val loss: 0.047 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.002 | Val loss: 0.047 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.002 | Val loss: 0.047 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.002 | Val loss: 0.048 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.001 | Val loss: 0.049 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.001 | Val loss: 0.049 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.001 | Val loss: 0.050 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.001 | Val loss: 0.051 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.001 | Val loss: 0.052 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.001 | Val loss: 0.053 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.001 | Val loss: 0.053 | Gen: ethay airway ondititingcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ondititingcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULCMHm5ZF7vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "79ca5f31-671e-443c-86a4-e5a564545d5a"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "\n",
        "TEST_SENTENCE = 'this string should be relatively difficult to explain to anthropologists'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ondititingcay isway orkingway\n",
            "source:\t\tthis string should be relatively difficult to explain to anthropologists \n",
            "translated:\tisthay ingstray ouldshay ebay elativelyray ifficultday otay explainway otay anthropogistsway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szNDNbJ6D5Hr",
        "colab_type": "text"
      },
      "source": [
        "## Transformer Non Causal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx8TxRL0FAlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2247
        },
        "outputId": "29c05657-d9f0-43c8-bc00-3486fe506bd8"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer_non_causal', # options: rnn / rnn_attention / transformer / transformer_non_causal\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_non_causal_encoder, transformer_non_causal_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_non_causal_encoder, transformer_non_causal_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                 num_transformer_layers: 3                                      \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: transformer_non_causal                 \n",
            "                               lr_decay: 0.99                                   \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TransformerDecoderNonCausal. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.128 | Val loss: 1.645 | Gen: e   iiiiiiiiiiiiiiiiiiii \n",
            "Epoch:   1 | Train loss: 1.518 | Val loss: 1.344 | Gen: ettttttttttttttttttt  ffffffffffffffffffff i ffffffffffffffffffff\n",
            "Epoch:   2 | Train loss: 1.227 | Val loss: 1.063 | Gen: eheheheheheheheheheh airrrrrrrrrrrrrrrrrr inininininininininin isdisdisdisdisdisdis oray\n",
            "Epoch:   3 | Train loss: 0.974 | Val loss: 0.873 | Gen: epay  ongay i oray  \n",
            "Epoch:   4 | Train loss: 0.790 | Val loss: 0.789 | Gen: ecay   isdisdisdisdisdisdis oray\n",
            "Epoch:   5 | Train loss: 0.659 | Val loss: 0.662 | Gen:  ayEOSayEOSayEOSarrrrrrrrrr  is orrrrrrrrrrrrrrrrrrr\n",
            "Epoch:   6 | Train loss: 0.612 | Val loss: 0.627 | Gen:   ondiondiondiondiondi is orkgway\n",
            "Epoch:   7 | Train loss: 0.547 | Val loss: 0.575 | Gen: etay ay ondititititititititi isway orrrrrrrrrrrrrrrrrrr\n",
            "Epoch:   8 | Train loss: 0.520 | Val loss: 0.543 | Gen:   ongcay isway orkay\n",
            "Epoch:   9 | Train loss: 0.469 | Val loss: 0.526 | Gen:    isway orkwaay    \n",
            "Epoch:  10 | Train loss: 0.438 | Val loss: 0.500 | Gen:    isway            \n",
            "Epoch:  11 | Train loss: 0.427 | Val loss: 0.497 | Gen: etay   isway orkingway\n",
            "Epoch:  12 | Train loss: 0.422 | Val loss: 0.487 | Gen: etay ayEOSway  isway orkingy\n",
            "Epoch:  13 | Train loss: 0.417 | Val loss: 0.511 | Gen:  airairairairairairai  isway orkingway\n",
            "Epoch:  14 | Train loss: 0.405 | Val loss: 0.479 | Gen: etay airairairairairairai  isway orkingway\n",
            "Epoch:  15 | Train loss: 0.404 | Val loss: 0.446 | Gen: etay   isway orkingway\n",
            "Epoch:  16 | Train loss: 0.394 | Val loss: 0.456 | Gen: etay ay ondiondiondiondiondi isway orkingway\n",
            "Epoch:  17 | Train loss: 0.394 | Val loss: 0.442 | Gen: etay ay  isway orkigwaay\n",
            "Epoch:  18 | Train loss: 0.382 | Val loss: 0.451 | Gen: etay  ondiondiondiondiondi isway orkingway\n",
            "Epoch:  19 | Train loss: 0.389 | Val loss: 0.436 | Gen: ethay ay  isway orkingway\n",
            "Epoch:  20 | Train loss: 0.363 | Val loss: 0.409 | Gen: etay airairairairairairai ongcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.354 | Val loss: 0.433 | Gen:   ondiondiondiondiondi isway orkingway\n",
            "Epoch:  22 | Train loss: 0.357 | Val loss: 0.416 | Gen: etay ay ondiondiondiondiondi isway orkingway\n",
            "Epoch:  23 | Train loss: 0.358 | Val loss: 0.418 | Gen: etay ay  isway orkingway\n",
            "Epoch:  24 | Train loss: 0.356 | Val loss: 0.399 | Gen:     orkingway       \n",
            "Epoch:  25 | Train loss: 0.361 | Val loss: 0.420 | Gen: etay airairairairairairai ondiondiondiondiondi isway orkingway\n",
            "Epoch:  26 | Train loss: 0.361 | Val loss: 0.422 | Gen: ethy airwiirwirwwirwirrwi  isway orkingway\n",
            "Epoch:  27 | Train loss: 0.356 | Val loss: 0.405 | Gen: ethy airairairairairairai  isway orkingway\n",
            "Epoch:  28 | Train loss: 0.349 | Val loss: 0.410 | Gen: etay airairairairairairai ondiondiondiondiondi isway orkingway\n",
            "Epoch:  29 | Train loss: 0.353 | Val loss: 0.404 | Gen: ethay ayEOSay  isway orkingway\n",
            "Epoch:  30 | Train loss: 0.348 | Val loss: 0.412 | Gen: ethay airwirwwrrwwrrwwrwww  y orkingway\n",
            "Epoch:  31 | Train loss: 0.337 | Val loss: 0.393 | Gen: ethay airairairairairairai  isway orkingway\n",
            "Epoch:  32 | Train loss: 0.345 | Val loss: 0.409 | Gen:  airairairairairairaa  isway orkingway\n",
            "Epoch:  33 | Train loss: 0.337 | Val loss: 0.388 | Gen: ethay   isway       \n",
            "Epoch:  34 | Train loss: 0.337 | Val loss: 0.392 | Gen:  ay  k orkingway    \n",
            "Epoch:  35 | Train loss: 0.344 | Val loss: 0.401 | Gen:  ay  isway          \n",
            "Epoch:  36 | Train loss: 0.351 | Val loss: 0.402 | Gen:  ay  isway          \n",
            "Epoch:  37 | Train loss: 0.345 | Val loss: 0.402 | Gen: ethay ay  isway     \n",
            "Epoch:  38 | Train loss: 0.336 | Val loss: 0.391 | Gen:    isway            \n",
            "Epoch:  39 | Train loss: 0.345 | Val loss: 0.396 | Gen: ethay ay  isway orkingway\n",
            "Epoch:  40 | Train loss: 0.332 | Val loss: 0.395 | Gen: ethay ay  isway orkingway\n",
            "Epoch:  41 | Train loss: 0.337 | Val loss: 0.385 | Gen:  ay  isway y        \n",
            "Epoch:  42 | Train loss: 0.324 | Val loss: 0.386 | Gen:  ay  isway orkingway\n",
            "Epoch:  43 | Train loss: 0.337 | Val loss: 0.393 | Gen:  ay  isway orkiny   \n",
            "Epoch:  44 | Train loss: 0.335 | Val loss: 0.392 | Gen:  ay  isway orkingway\n",
            "Epoch:  45 | Train loss: 0.331 | Val loss: 0.377 | Gen: ethay ay  isway orkiny\n",
            "Epoch:  46 | Train loss: 0.318 | Val loss: 0.373 | Gen:    isway orkingway  \n",
            "Epoch:  47 | Train loss: 0.314 | Val loss: 0.370 | Gen: ethay ay  isway orkiny\n",
            "Epoch:  48 | Train loss: 0.310 | Val loss: 0.377 | Gen: ethay   isway orkingway\n",
            "Epoch:  49 | Train loss: 0.317 | Val loss: 0.370 | Gen: ethay ay  isway orkiny\n",
            "Epoch:  50 | Train loss: 0.318 | Val loss: 0.377 | Gen: ethay   isway orkingway\n",
            "Epoch:  51 | Train loss: 0.322 | Val loss: 0.378 | Gen: ethay ay  isway orkiny\n",
            "Epoch:  52 | Train loss: 0.337 | Val loss: 0.380 | Gen: ethay ay  isway y   \n",
            "Epoch:  53 | Train loss: 0.318 | Val loss: 0.382 | Gen: ethay ay y isway y  \n",
            "Epoch:  54 | Train loss: 0.316 | Val loss: 0.376 | Gen: ethay ay  isway orkingway\n",
            "Epoch:  55 | Train loss: 0.315 | Val loss: 0.365 | Gen: ethay ay y isway y  \n",
            "Epoch:  56 | Train loss: 0.309 | Val loss: 0.362 | Gen: ethay   isway orkiny\n",
            "Epoch:  57 | Train loss: 0.309 | Val loss: 0.366 | Gen: ethay ay y isway y  \n",
            "Epoch:  58 | Train loss: 0.310 | Val loss: 0.366 | Gen: ethay ay  isway y   \n",
            "Epoch:  59 | Train loss: 0.308 | Val loss: 0.363 | Gen: ethay ay  isway oy  \n",
            "Epoch:  60 | Train loss: 0.311 | Val loss: 0.371 | Gen: ethay airwairwairwairwairw  isway y\n",
            "Epoch:  61 | Train loss: 0.317 | Val loss: 0.361 | Gen: ethay airairairairairairai  isway y\n",
            "Epoch:  62 | Train loss: 0.309 | Val loss: 0.365 | Gen: ethay ay  isway     \n",
            "Epoch:  63 | Train loss: 0.304 | Val loss: 0.360 | Gen: ethay airairairairairairai  isway orkiny\n",
            "Epoch:  64 | Train loss: 0.303 | Val loss: 0.367 | Gen: ethay ay  isway     \n",
            "Epoch:  65 | Train loss: 0.306 | Val loss: 0.367 | Gen: ethay ay  isway     \n",
            "Epoch:  66 | Train loss: 0.305 | Val loss: 0.366 | Gen: ethay ay  isway orkiny\n",
            "Epoch:  67 | Train loss: 0.306 | Val loss: 0.365 | Gen: ethay ay  isway y   \n",
            "Epoch:  68 | Train loss: 0.306 | Val loss: 0.372 | Gen: ethay ayEOSway  isway orkiny\n",
            "Epoch:  69 | Train loss: 0.307 | Val loss: 0.360 | Gen: ethay ay  isway y   \n",
            "Epoch:  70 | Train loss: 0.311 | Val loss: 0.368 | Gen: ethay ay  isway     \n",
            "Epoch:  71 | Train loss: 0.304 | Val loss: 0.352 | Gen: ethay ay  isway y   \n",
            "Epoch:  72 | Train loss: 0.303 | Val loss: 0.357 | Gen: ethay ay  isway orkiny\n",
            "Epoch:  73 | Train loss: 0.303 | Val loss: 0.355 | Gen: ethay ay  isway     \n",
            "Epoch:  74 | Train loss: 0.305 | Val loss: 0.363 | Gen:    isway            \n",
            "Epoch:  75 | Train loss: 0.304 | Val loss: 0.356 | Gen: ethay ay  isway     \n",
            "Epoch:  76 | Train loss: 0.297 | Val loss: 0.354 | Gen:    isway            \n",
            "Epoch:  77 | Train loss: 0.293 | Val loss: 0.356 | Gen: ethay ay  isway     \n",
            "Epoch:  78 | Train loss: 0.292 | Val loss: 0.352 | Gen: ethay   isway       \n",
            "Epoch:  79 | Train loss: 0.295 | Val loss: 0.357 | Gen: ethay   isway       \n",
            "Epoch:  80 | Train loss: 0.295 | Val loss: 0.350 | Gen: ethay ay  isway     \n",
            "Epoch:  81 | Train loss: 0.296 | Val loss: 0.352 | Gen: ethay   isway       \n",
            "Epoch:  82 | Train loss: 0.296 | Val loss: 0.349 | Gen: ethay   isway       \n",
            "Epoch:  83 | Train loss: 0.297 | Val loss: 0.355 | Gen:  ay  isway          \n",
            "Epoch:  84 | Train loss: 0.299 | Val loss: 0.353 | Gen: ethay ay  isway y   \n",
            "Epoch:  85 | Train loss: 0.299 | Val loss: 0.356 | Gen: ethay ayEOSaay  isway y\n",
            "Epoch:  86 | Train loss: 0.297 | Val loss: 0.347 | Gen: ethay ay  isway     \n",
            "Epoch:  87 | Train loss: 0.293 | Val loss: 0.349 | Gen: y ayEOSway  isway y \n",
            "Epoch:  88 | Train loss: 0.294 | Val loss: 0.346 | Gen:    isway            \n",
            "Epoch:  89 | Train loss: 0.294 | Val loss: 0.345 | Gen:  airwairwairwairwairw  isway y\n",
            "Epoch:  90 | Train loss: 0.295 | Val loss: 0.341 | Gen:    isway            \n",
            "Epoch:  91 | Train loss: 0.295 | Val loss: 0.357 | Gen: ethay ay  isway     \n",
            "Epoch:  92 | Train loss: 0.301 | Val loss: 0.341 | Gen: ethay ay  isway y   \n",
            "Epoch:  93 | Train loss: 0.293 | Val loss: 0.346 | Gen: ethay ay  isway     \n",
            "Epoch:  94 | Train loss: 0.295 | Val loss: 0.343 | Gen: ethay airairairairairairai  isway \n",
            "Epoch:  95 | Train loss: 0.292 | Val loss: 0.350 | Gen:    isway            \n",
            "Epoch:  96 | Train loss: 0.292 | Val loss: 0.346 | Gen: ethyy   isway       \n",
            "Epoch:  97 | Train loss: 0.286 | Val loss: 0.337 | Gen:    isway            \n",
            "Epoch:  98 | Train loss: 0.285 | Val loss: 0.344 | Gen: ethay ayEOSway  isway \n",
            "Epoch:  99 | Train loss: 0.287 | Val loss: 0.338 | Gen: ethyy ay  isway     \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethyy ay  isway \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q76ELY4oE2fg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7f9d4191-ea15-421e-f7e1-7a32fa9e4562"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_non_causal_encoder, transformer_non_causal_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "\n",
        "TEST_SENTENCE = 'this string should be relatively difficult to explain to anthropologists'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_non_causal_encoder, transformer_non_causal_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethyy ay  isway \n",
            "source:\t\tthis string should be relatively difficult to explain to anthropologists \n",
            "translated:\ty     iculfdfyfiaulfdfyfif  explainwainwainwaiww  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbfZCByITOI6",
        "colab_type": "text"
      },
      "source": [
        "# Attention visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBv4QQuBiU-V",
        "colab_type": "text"
      },
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXvqoQYONMTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "bc3aa8eb-26c4-45b0-8d50-9a5f59a13f75"
      },
      "source": [
        "TEST_WORD_ATTN = 'cake'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "translated = translate_sentence(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_WORD_ATTN, translated))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEYCAYAAACdnstHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1JpJRJLUmklEktTa/weUXgm5ra9KIwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "source:\t\tcake \n",
            "translated:\takecay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUFHC0ITRZTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "3fe40a3b-8516-4c62-cb86-e43fd47fbc9c"
      },
      "source": [
        "TEST_WORD_ATTN = 'well-mannered'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "translated = translate_sentence(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_WORD_ATTN, translated))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "source:\t\twell-mannered \n",
            "translated:\tellway-annermedmay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90W1bkgYTQP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "5a3c7517-6309-4822-c05a-21c67a47f1bf"
      },
      "source": [
        "TEST_WORD_ATTN = 'string'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "translated = translate_sentence(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_WORD_ATTN, translated))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niBOImZkV8l9LskOqwgZeVAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "source:\t\tstring \n",
            "translated:\tingstray\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSrYtTC5TkVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "28e22964-5a5d-42b4-c793-fe8b41c89656"
      },
      "source": [
        "TEST_WORD_ATTN = 'anthropologists'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "translated = translate_sentence(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_WORD_ATTN, translated))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAEYCAYAAAD76PVVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "source:\t\tanthropologists \n",
            "translated:\tanthropoogistsway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuOvxfA1NMz3",
        "colab_type": "text"
      },
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSSB4wd8-M7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "outputId": "aaea57f2-d9e0-404b-dde7-d3a370ea35d0"
      },
      "source": [
        "TEST_WORD_ATTN = 'cake'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )\n",
        "translated = translate_sentence(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_WORD_ATTN, translated))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEYCAYAAACdnstHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAHZxJREFUeJzt3Xm8XWV97/HP95yEhEGGEKw0REAM\nUERlCJOIIgKNrS+QAiVQi7Ta1F5BkJcD9NJcS0VRbievWIlcylAlt4BICqmBIqNMSSCACQIxMoTX\n7YVAAC1DCPndP9ZzcLM9e1jr7H3WWud833mtV9a0n/Xb++y9f/tZz1rPo4jAzMysiIGyAzAzs/py\nEjEzs8KcRMzMrDAnETMzK8xJxMzMCnMSMTOzwpxEzMysMCcRMzMrzEnEzMwKm1B2AGZm9ptmzZoV\na9asyfWYpUuXLoqIWX0KaVhOIqNE0g7AtRGxe8mhmFkNrFmzhiVLluR6jKSpfQqnJScRM7OKqkPf\nhrVsE5H0Q0lLJS2XNKfsePKS9A5J90nap+xY2pH0cUn3SFom6QJJg2XH1ImkEyU9IOl+SZeVHU83\n6vZ+lnS2pNMals+RdGqZMY1VGyJyTWWoa03kTyPiOUkbA4slXRURz5YdVDck7QLMB06KiPvLjqcV\nSb8DHAccGBGvSfo28EfApeVG1pqkdwFnAe+LiDWSppQdU5fq9n6+CPgB8A+SBoDZwL7lhjT2BPWo\nidQ1iXxW0lFpfjowA6jyh27INsA1wB9ExIqyg+ngw8DeZF9qABsDT5caUWeHAFdExBqAiHiu5Hi6\nVav3c0Q8JulZSXsCvwXcV/GkV1NB4CTSc5IOBg4FDoiIlyTdDEwuNajuvQA8AbwfqHoSEXBJRJxZ\ndiBjWY3fzxcCJwFvI6uZWK8FbKh+Dqllm8gWwNr0gdsV2L/sgHJYBxwFnCjphLKD6eBG4BhJbwWQ\nNEXS9iXH1MmPgWMlbQ1ZzCXH0426vp+vBmYB+wCLSo5lzIqIXFMZalcTAX4EfFrSQ8DDwF0lx5NL\nRPyXpI8CN0j6VUQsKDum4UTECklnAden896vAZ8BHi83stYiYrmkc4BbJL0O3Ef2a7nKavl+joh1\nkm4Cno+I18uOZywKKK2xPA/VoeHGzKol/bC4Fzg2Ih4tO56xaK+9947b7rgj12M2mzx5aUTM7FNI\nw6rj6SwzK5Gk3YCVwI1OIP3l01lmNuakKwvfUXYcY12UeO9HHk4iZmYVVYfmBicRM7OKqsN9IrVu\nE6lDFxHNHPPocMz9V7d4oV4xZ1dn5ZvKUOskAtTmDdHAMY8Ox9x/dYsXahazG9bNzKwwN6wDAwOD\nMTjYn8MMDAwyceKknr/K69ev63WRbyKp+u+MJo45MzDQv46MpQEGByf0PObU91nPDQwMMGHCxL68\nLyYMTuxHsUyYMJHJkzbpS8yvrnt5TURs07MC+1C7kDQL+EdgELgwIs5t2v73wIfS4ibAWyNiy3Zl\n9j2JDA5OYMqUbft9mJ56+unK3pTdRn++KPppYKB+Z1M32WTzskPIbeLESWWHkNuUrd5Wdgi5/XzV\nsp5+cfS6F980lMP5wGHAarLOVRc0dgYbEZ9r2P8UYM9O5dbvU2xmNk70eDyRfYGVEbEqItaRDUlx\nZJv9jwcu71So20TMzCqqQE1kqqTGMXXnRcS8ND8NeLJh22pgv+EKSZ2t7kjWqWlbTiJmZpVUaDyR\nNT3qO2s2cGU3nWs6iZiZVVD0/t6Pp8gGPRuyXVo3nNlkvXZ35CRiZlZRPb46azEwQ9KOZMljNvAb\n4xqlcW22Au7splAnETOziuplEomI9ZJOJhtEbBC4KI3BczawpGFso9nA/Ojy4E4iZmYV1I9BqSJi\nIbCwad3cpuUv5ynTScTMrKLci6+ZmRXj8UTMzGwkXBMxM7NCgnqMJ+IkYmZWUWWNEZKHk4iZWUX5\ndJaZmRXmJGJmZoWEr84yM7ORcE3EzMwKcxIxM7NC+tHtST84iZiZVVQd7hPpanhcSZdI2rJheStJ\nF/UvLDMz2xD5pjJ0WxN5T0Q8P7QQEWsldRzA3czMCooYU20iA5K2ioi1AJKmtHuspDnAHICBgcER\nB2lmNt4EY6th/W+BOyVdkZaPBc5ptXMaGH4ewMSJk6r/KpiZVdCYaViPiEslLQEOSav+ICJW9C8s\nMzMbSzURUtJw4jAzGyVjKomYmdnocbcnZmY2InW4T8RJxMysojyeiJmZFTLWLvE1M7NR5iRiZmaF\nuWHdzMyKqUm3J111wGhmZqNrqE0kz9SJpFmSHpa0UtIZLfb5Q0krJC2X9P1OZbomYmZWUb08nSVp\nEDgfOAxYDSyWtKCx9xFJM4AzgQNTR7tv7VSuayJmZhUVOf91sC+wMiJWRcQ6YD5wZNM+fwacP9TZ\nbkQ83alQJxEzs4qKyDcBUyUtaZjmNBQ3DXiyYXl1WtdoZ2BnST+RdJekWZ1i9OksM7MKKjg87pqI\nmDmCw04AZgAHA9sBt0p6d+N4UsM9wMzMqqb3V2c9BUxvWN4urWu0Grg7Il4DfiHpEbKksrhVoT6d\nZWZWURtSJ4zdTh0sBmZI2lHSRsBsYEHTPj8kq4UgaSrZ6a1V7Qp1TcTMrIJ63e1JRKyXdDKwCBgE\nLoqI5ZLOBpZExIK07XBJK4DXgS9ExLPtynUSMTMbJyJiIbCwad3chvkATk9TV5xEzMwqqg53rDuJ\nmJlVlPvOAjbffAqH/u4J/T5MT13/75eVHUJur776UtkhjAsvvtj29HAlfe27l5cdQm5XfufiskOo\ngK5uICydayJmZhXUcANhpTmJmJlVlE9nmZlZYW5YNzOzQgp2ezLqnETMzCrKNREzMyumJiMbOomY\nmVWVk4iZmRUVG5xEzMysoBpURJxEzMyqKLvZsPpZxEnEzKyinETMzKwgX51lZmYj4IZ1MzMrxG0i\nZmY2Ik4iZmZWnJOImZkVVYMc4iRiZlZJEW5YNzOz4sZUm4ikrYAZwOShdRFxaz+CMjMb74IxlEQk\nfQo4FdgOWAbsD9wJHNK/0MzMxrc6JJGBLvc7FdgHeDwiPgTsCTzft6jMzIxIY4p0O5Wh2yTySkS8\nAiBpUkT8DNil1c6S5khaImnJK6++1Is4zczGlwjYkHPqQNIsSQ9LWinpjGG2nyTpGUnL0vSpTmV2\n2yayWtKWwA+BGyStBR5vtXNEzAPmAWy99bbVr4+ZmVVQL2sXkgaB84HDgNXAYkkLImJF067/JyJO\n7rbcrpJIRByVZr8s6SZgC+BH3R7EzMzy6/EZqn2BlRGxCkDSfOBIoDmJ5JL7Et+IuGUkBzQzs876\ncHXWNODJhuXVwH7D7He0pA8AjwCfi4gnh9nnDd22iZiZ2WiKQg3rU4fao9M0J+dR/w3YISLeA9wA\nXNLpAb7Z0Mysogrcsb4mIma22PYUML1hebu07tfHi3i2YfFC4BudDuiaiJlZJeWrhXRx6msxMEPS\njpI2AmYDCxp3kLRtw+IRwEOdCnVNxMysonrZJhIR6yWdDCwCBoGLImK5pLOBJRGxAPispCOA9cBz\nwEmdynUSMTOroH4MShURC4GFTevmNsyfCZyZp0wnETOzqqpBtydOImZmFRUbyo6gMycRM7OKqkMH\njE4iZmZVVGKnink4iZiZVZSTiJmZFTKmBqUyM7NRFoXuWB91TiJmZlXlmoiZmRXjhnUzMxuBGuQQ\nJxEzs6pyTcTMzAoJN6ybmdlIuCYCvPbaOv7v6if6fZieuvOnS8oOIbdPHXda2SHk9uKLz3beqWJ2\n3/2gskPIbfLkTcsOIbf9PnhI2SHkdu+91/e8TCcRMzMryFdnmZlZUX0YT6QfnETMzKrKDetmZlZE\n1ndW2VF05iRiZlZRPp1lZmbFeDwRMzMbCd9saGZmhbkmYmZmhXhQKjMzK64ml2c5iZiZVVI9GtYH\nyg7AzMyGFxvyTZ1ImiXpYUkrJZ3RZr+jJYWkmZ3KdE3EzKyielkTkTQInA8cBqwGFktaEBErmvZ7\nC3AqcHc35bomYmZWRanvrDxTB/sCKyNiVUSsA+YDRw6z398AXwde6SZMJxEzswoaujqrh0lkGvBk\nw/LqtO4NkvYCpkfEdd3G6dNZZmYVVeB01lRJjQMizYuIed08UNIA8HfASXkO6CRiZlZJUeSO9TUR\n0aox/ClgesPydmndkLcAuwM3SwJ4G7BA0hER0XKkPicRM7Mq6v14IouBGZJ2JEses4ET3jhcxAvA\n1KFlSTcDn2+XQMBtImZm1RWRb2pbVKwHTgYWAQ8B/xoRyyWdLemIoiF2VRORtHdELG1a99GIuLbo\ngc3MrL1e32sYEQuBhU3r5rbY9+Buyuy2JvJdSbsPLUg6HvirLh9rZmY59eHqrL7otk3kGOBKSScA\nBwEnAof3LSozs/EuxlBX8BGxStJs4IfAE8DhEfFyq/0lzQHmAEyatEkv4jQzG2fq0XdW2yQi6UGy\nWtWQKcAgcLckIuI9wz0uXZc8D2Dzzbeu/qtgZlZBtU8iwEdHJQozM/sNtU8iEfH4aAViZmZN6p5E\nzMysHDGWGtbNzGz01aAi4iRiZlZNY+DqLDMzK4+TiJmZFdP7Dhj7wknEzKyCAjesm5nZCLgmYmZm\nBXXu3r0KnETMzKrIbSJmZjYSNcghTiJmZlXlhnUzMytkaFCqqvMY62ZmVphrImZmVeSGdTMzK859\nZ5mZ2Qg4iZiZWWG+OsvMzIrJLs8qO4qO+p5EXn99Pb/85XP9PkxPzTrwI2WHkNt1t11Xdgi57b/b\nnmWHkNsLLzxTdgi5Td16Wtkh5Lb2mv8sO4TS1SSH+BJfM7OqiohcUyeSZkl6WNJKSWcMs/3Tkh6U\ntEzS7ZJ261Smk4iZWSXlSyCdkoikQeB84CPAbsDxwySJ70fEuyNiD+AbwN91itJtImZmVRQ9b1jf\nF1gZEasAJM0HjgRWvHHIiBcb9t80i6I9JxEzs4rq8SW+04AnG5ZXA/s17yTpM8DpwEbAIZ0K9eks\nM7MKGuo7K+fprKmSljRMc3IfN+L8iNgJ+BJwVqf9XRMxM6uoAjWRNRExs8W2p4DpDcvbpXWtzAf+\nqdMBXRMxM6ukNLJhnqm9xcAMSTtK2giYDSxo3EHSjIbF3wce7VSoayJmZlUUEBt6WFzEekknA4uA\nQeCiiFgu6WxgSUQsAE6WdCjwGrAW+ESncp1EzMwqqtd9Z0XEQmBh07q5DfOn5i3TScTMrKLcAaOZ\nmRVSl5ENnUTMzKrIg1KZmVlx4a7gzcxsBFwTMTOzoqJz11WlcxIxM6ugcJuImZkVF0Qv7zbsEycR\nM7OKck3EzMwKq0MS6aoDRmU+LmluWn67pH37G5qZ2fjW6+Fx+6HbXny/DRwAHJ+Wf0k2zKKZmfVB\nlhg25JrK0O3prP0iYi9J9wFExNrUlbCZmfVLDU5ndZtEXkuDvAeApG2AlmkvjaY1B2CjjSaPNEYz\ns3GpDveJdHs665vA1cBbJZ0D3A58tdXOETEvImZGxMwJE1xhMTMrog5tIl3VRCLie5KWAh8GBHws\nIh7qa2RmZuNcHa7O6voS34j4GfCzPsZiZmZv8M2GZmZWkLs9MTOzEXESMTOzwpxEzMysoBhT94mY\nmdkoi9a341WGk4iZWUX5dJaZmRXiq7PMzGwEyrsLPQ8nETOziqrDzYbd9p1lZmajrNd9Z0maJelh\nSSslnTHM9tMlrZD0gKQbJW3fqUwnETOziuplEkk9sZ8PfATYDThe0m5Nu90HzIyI9wBXAt/oFKOT\niJlZFUXkn9rbF1gZEasiYh0wHzjyzYeMmyLipbR4F7Bdp0LdJmJmVkFBofFEpkpa0rA8LyLmpflp\nwJMN21YD+7Up65PAv3c6oJOImVlFFWhYXxMRM0d6XEkfB2YCH+y0r5OImVkl9fwS36eA6Q3L26V1\nbyLpUOC/Ax+MiFc7FeokYmZWUT1OIouBGZJ2JEses4ETGneQtCdwATArIp7uplAnETOziuplEomI\n9ZJOBhYBg8BFEbFc0tnAkohYAJwHbAZcIQngiYg4ol25TiJmZhWUXXDV25sNI2IhsLBp3dyG+UPz\nltn3JDIwMMjGG2/W78P01Nq1ZUeQ336/896yQ8jtkSceLTuE3HZ42/TOO1XMe957cNkhjAurVt3f\n4xLd7YmZmY2Ek4iZmRVV4D6RUeckYmZWUT6dZWZmBUUtevF1EjEzqyAPSmVmZiPiJGJmZoU5iZiZ\nWWFOImZmVlCAG9bNzKwo3ydiZmaF+OosMzMbEScRMzMryDcbmpnZCLgmYmZmhTmJmJlZIW5YNzOz\nEQiPJ2JmZsUFblg3M7OCfDrLzMwKq0MSGWi3UdI+kt7WsHyipGskfVPSlP6HZ2Y2XgUR+aYytE0i\nwAXAOgBJHwDOBS4FXgDm9Tc0M7PxK7s6a0OuqQydTmcNRsRzaf44YF5EXAVcJWlZqwdJmgPMAZg0\naZOeBGpmNt7U/nQWMChpKNF8GPhxw7aWCSgi5kXEzIiYOXHipJHGaGY2Lo2F01mXA7dIugZ4GbgN\nQNI7yU5pmZlZX8TQOa3upw4kzZL0sKSVks4YZvsHJN0rab2kY7qJsu3prIg4R9KNwLbA9fHrVDcA\nnNLNAczMrJhejiciaRA4HzgMWA0slrQgIlY07PYEcBLw+W7L7XiJb0TcJelDwJ9IAlgeETfliN3M\nzArocWP5vsDKiFgFIGk+cCTwRhKJiMfStq4P3DaJSJoG/AB4BViaVh8r6evAURHxVI4nYGZmXSrY\nd9ZUSUsaludFxNCVtNOAJxu2rQb2G0GIQOeayLeAf4qIixtXSjoR+DZZFjMzs54r1Fi+JiJm9iOa\nVjo1rO/WnEAAIuJSYNe+RGRmZkDPr856CpjesLxdWjcinWoiwyYZSQPA4EgPbmZmrfX4st3FwAxJ\nO5Ilj9nACSMttFNN5FpJ35W06dCKNP8dYOFID25mZq318o71iFgPnAwsAh4C/jUilks6W9IR8EZX\nV6uBY4ELJC3vFGOnmsgXga8Bj0t6PK17O3AJ8JedCjczs4K6vPcjX5GxkKYKQETMbZhfTHaaq2ud\n7hN5Dfi8pL8C3plW/zwiXspzEDMzyyfo7X0i/dKpF98vAkTEy8CuEfHgUAKR9NVRiM/MbNwaC92e\nzG6YP7Np26wex2JmZg3q0ItvpySiFvPDLZuZ2TjTqWE9WswPt2xmZj1T3imqPDolkfdKepGs1rFx\nmictT+5rZGZm41ztk0hE+IZCM7MSFOw7a9R17MXXzMzK4SRiZmYFBZR0xVUeTiJmZhVVh5sNnUTM\nzCqqDqez1O8gJT0DPN5xx2KmAmv6VHa/OObR4Zj7r27xQn9j3j4itulVYRMmTIzNNtsq12NeeOGZ\npaM9nkjfayK9fFGbSVoy2i/YSDnm0eGY+69u8UK9Ys66MnGbiJmZFVSH01lOImZmFeUk0n/zOu9S\nOY55dDjm/qtbvFCzmOuQRDp1wFhpEdH2DSHpY5JC0q4N63aQdELD8h6Sfm8kcUj6y6blO4rG3AuS\njpB0Rod9DpZ0bYttp0naZGi5y9d5t5wxvukxkm6W1LNz1aPxOvfacDE3/y1GUxrx7tBW25vjbfee\n6hdJF0l6WtJPu9m/du+LoYGpup1KUOsk0oXjgdvT/0N24M3jCu8BjCiJ0DTKY0S8b4TljUhELIiI\nc0dQxGlAni+ujwG5kkjBx4waSVWppef9W/RMRMyNiP8o49jDafE3uZgxOyxFEGzINZVhzCYRSZsB\n7wc+yZvHRTkXOEjSMklfAs4GjkvLx0naNP26uUfSfZKOTOWdJOkHkn4k6VFJ30jrzyXrnHKZpO+l\ndb9K/0vSeZJ+KulBScel9QenX95XSvqZpO9JelPX+pLeKmlpmn9vqlG9PS3/XNImkraRdJWkxWk6\nsCHWb6X5nSTdlY7/laHYks2aY5D0WeC3gZsk3SRpUNLFDc/hc01xvg84AjgvvQY7pdrdXZIekHS1\npK06PSZtOja97o9IOijtO5hew8WpvD8f5m+9qaTrJN2f4hx6nT+c/oYPpr/ppLT+MUlT0/xMSTen\n+S9LukzST4DL0rH/ZyrzAUmnpP32lnSLpKWSFknadpiYjk2Pu1/Sre2eS6v3Q/PfIu17uKQ7Jd0r\n6Qpl7/Oh5/TXaf2DSrVvSZtJ+ue07gFJR7crp+k5XCzpmHbltyJp31T+fZLukLRLWn+rpD0a9rtd\n2fu73edugaQfAzc2HycibgWeaxdLXQ31nVX1QalyB1mXCfgj4H+n+TuAvdP8wcC1DfudBHyrYfmr\nwMfT/JbAI8Cmab9VwBZkPRg/DkxP+/2q6di/Sv8fDdwADAK/BTwBbJtieIFsLOMB4E7g/cM8h+XA\n5sDJwOL0nLYH7kzbvz/0OODtwEPNzwm4Fjg+zX+6IbaWMQCPAVPT/N7ADQ0xbTlMnBcDxzQsPwB8\nMM2fDfxDF4+5GfjbNP97wH+k+TnAWWl+ErAE2LGprKOB7zYsD/2NngR2TusuBU4b5vnNBG5O818G\nlgIbp+W/AK4EJqTlKcBEsvfTNmndccBFwzy/B4Fpja9Zq+eS428xFbgV2DQtfwmY27DfKWn+vwEX\npvmvN77+wFbtymn1N2pVftP+B5M+W2Tv26HX7VDgqjT/iaF4gJ2BJV187lYDU9p81ncAflr2d06v\nJ2kgJk/eNNc09HqO5jRmayJkp7Dmp/n5vPmUVjuHA2dIWkb2xTaZ7Asa4MaIeCEiXgFWkH2ht/N+\n4PKIeD0i/h9wC7BP2nZPRKyO7ELwZWQfhGZ3AAcCHyD7kH0AOAi4LW0/FPhWinUBsPkwvygPAK5I\n899v2tZNDKuAd0j6X5JmAS8Os88bJG1B9qV5S1p1SYq7Gz9I/y9tiOVw4MT0HO8GtgZmND3uQeAw\nSV+XdFBEvADsAvwiIh7JGceCyIaDhuz1vSAi1gNExHOp3N2BG1JMZ5F9+Tf7CXCxpD8j+xHR6bl0\n87fYn+wU4E9SGZ/gze/B4V6/Q4Hzh3aIiLVdlNPKcOW3sgVwhbK2ir8H3pXWXwF8VNJE4E/JEhW0\n/9zdkF77cSfvF3oZqnLet6ckTQEOAd4tKcg+xCHpC908HDg6Ih5uKnM/4NWGVa8zstevm7JuJUsa\n2wPXkP1iDOC6tH0A2D8ltcZYexZDRKyV9F7gd8lqMn9I9uHvh6F4GmMR2S/gRa0eFBGPSNqLrAbz\nFUk3kr1erazn16dym8fF+a8OMQpYHhEHtNspIj6d3jO/DyyVtDctnoukg+nu/SCyL9RWP4iGe/1a\nPYd25bTSbfkAfwPcFBFHSdqBLDEQES9JugE4kuy9tHdDTK0+d53+JmNUeYkhj7FaEzkGuCwito+I\nHSJiOvALsi/kXwJvadi3eXkRcIrSN7GkPbs43mvpl1Wz28jaWwYlbUP2S/ieHM/jNuDjwKPpF+pz\nZF+Ut6ft1wOnDO3ceK65wV1kp3vgzW1D7bzxmqS2g4GIuIrsV/de7fZPtYC1Sm0awB+T1cBaPqaD\nRcBfDL2+knaWtGnjDpJ+G3gpIv4FOC/F+DCwg6R3DhPHY/z6y+toWrsB+HOlBt304+RhYBtJB6R1\nEyW9q/mBknaKiLsjYi7wDDC9m+cyjMbX6S7gwKHnlNoRdu7w+BuAzzTEtVXBcvLaAngqzZ/UtO1C\n4JvA4lQzgmKfuzFvLIyxXlfHA1c3rbsqrX8AeD01eH4OuAnYTalhnewX1ETgAUnL03In89L+32ta\nf3U63v3Aj4EvRsR/dvskIuIxsl9ot6ZVtwPPN3zwPgvMTA2mK8hqCs1OA06X9ADwTrJz7908nx+l\nxtxpwM3pNMO/AGcOs/984AupQXQnstMj56Vj7kHWLtLpMa1cSHbq8N50auQCfvNX8LuBe1KM/wP4\nSqqd/QnZKZUHgQ3Ad9L+fw38o6QlZL+q2x37CbK/7f3ACRGxjuxHytfTumXAcFfjnZcaoH9Kdlry\n/i6fS7M3/hYR8QzZF/Ll6bW9E2jbwA18BdhKqZEf+FDBcvL6BvA1SffR9BwjYinZadF/blhd5HOH\npMvJ4t9F0mpJn+xF8FUQNWlY73sHjFYuZfcYvBwRIWk2WSP7kWXHZeNXqjneDOwadegcqiTSQEwY\nzHfGfP3rr429DhitdHuTNb4LeJ7+tWeYdSTpROAc4HQnkE6iFuOJuCZiZlZBkmJgIF+Lw4YNG1wT\nMTOzTB1+5DuJmJlV06KImJrzMaM+SJhPZ5mZWWFj9RJfMzMbBU4iZmZWmJOImZkV5iRiZmaFOYmY\nmVlhTiJmZlaYk4iZmRXmJGJmZoU5iZiZWWH/H+jVi8HmOjP5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEYCAYAAACdnstHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEYCAYAAACdnstHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n//PvZt4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "source:\t\tcake \n",
            "translated:\takecay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlUvD_tYealI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "outputId": "071a7f68-cf31-4e62-8439-cd670bd96dfc"
      },
      "source": [
        "TEST_WORD_ATTN = 'anthropologists'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )\n",
        "translated = translate_sentence(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_WORD_ATTN, translated))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEYCAYAAAAK467YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEYCAYAAAAK467YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAdTMLJMDqJlZJgdQM7NM/x+4zcpn5Po0aQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEYCAYAAAAK467YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "source:\t\tanthropologists \n",
            "translated:\tanthropogistsway\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}